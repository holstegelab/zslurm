#!/usr/bin/env python
import curses, curses.textpad, curses.ascii
import os
import datetime
import subprocess, sys, getopt, time, shlex
import itertools
from subprocess import Popen

try:
    import _thread
    from xmlrpc.server import SimpleXMLRPCServer,SimpleXMLRPCRequestHandler
except:
    import thread as _thread
    from SimpleXMLRPCServer import SimpleXMLRPCServer,SimpleXMLRPCRequestHandler
import socket
import threading
from collections import defaultdict, OrderedDict, deque

import random
import math
import psutil
import numpy
import getpass
import traceback

import zslurm_shared
import atexit
import ipyparallel

mylocal_id = zslurm_shared.short_name(zslurm_shared.get_hostname())
PHASING_OUT = "PHASING_OUT"



# return codes
RC_CANCELLED = -255
RC_ENGINE_TERMINATED = -1
RC_SUCCESS = 0
RC_TERMINATED = 15

WHITE = 0
RED = 1
GREEN = 2
YELLOW = 3
MAGENTA = 4
BLUE = 5
CYAN = 6

TIMEOUT = 1200

# Reduce managed/unmanaged flapping by requiring consecutive squeue observations
# to (re)confirm managed state, and consecutive misses to consider unmanaged.
SQUEUE_OBSERVED_CONFIRMS = 2
SQUEUE_MISSING_TOLERANCE = 2


# current screen info
class gb:
    scr = None
    lock = None

    wlog = None
    wlog_currow = 0
    wlog_size = (0, 0)

    wcom = None
    wcom_status = None

    wstatus = None
    wstatus_progress = None


class Servers:
    manager_server = None
    job_server = None


class Status(object):
    def __init__(self):
        self.stop_status_display = False
        self.address = ""
        self.port = 0
        self.instance_name = None
        self.lastin_first = False
        self.prio_fillmem_context = 500
        self.used_load = numpy.array([], dtype=float)
        self.used_cpus = numpy.array([], dtype=float)
        self.used_mem = numpy.array([], dtype=float)
        self.total_cpus = numpy.array([], dtype=float)
        self.total_mem = numpy.array([], dtype=float)
        self.sys_cpu_busy = numpy.array([], dtype=float)
        self.sys_iowait = numpy.array([], dtype=float)
        self.autoconsolidate_enable = True
        self.consolidation_safety_fraction = 1.05
        self.consolidation_min_engines = 0
        self.consolidation_hysteresis_sec = 120.0
        self.consolidation_cooldown_sec = 180.0
        self.consolidation_min_runtime_sec = 300.0
        self.consolidation_keep_weights = {
            'jobs': 0.5,
            'reserves': 0.3,
            'time': 0.2,
            'reserves_alpha_cpu': 0.5,
        }
        self.consolidation_allow_with_queue = True
        self.consolidation_pending_min_age_sec = 60.0
        self.consolidation_debug = True
        self.consolidation_want = {}
        self.consolidation_last_action_ts = 0.0
        self.consolidation_last_action_ts_by_part = {}
        self.ssd_prefer_bonus = 0.25
        
        self.autogrow_enable = True
        self.autogrow_cooldown_sec = 500.0
        self.autogrow_runtime_default = "5-0:0:0"
        
        self.autogrow_last_action_compute_ts = 0.0
        self.autogrow_last_action_staging_ts = 0.0

        self.autogrow_prefer_partitions = [
            ("genoa", True),
            ("genoa", False),
            ("rome", True),
            ("rome", False),
            ("fat_genoa", True),
            ("fat_rome", True),
        ]
        self.autogrow_debug = True
        
        self.node_profiles = {
            "genoa": {"cores": 192, "mem_gb": 336},
            "rome": {"cores": 128, "mem_gb": 224},
            "fat_genoa": {"cores": 192, "mem_gb": 1440},
            "fat_rome": {"cores": 128, "mem_gb": 960},
        }
        self.node_reports_file = None
        self.reports_lock = threading.RLock()
        self.autogrow_plan = None        
        

    def set_address(self, address, port):
        self.address = address
        self.port = port


status = Status()


def parse_timeleft(time):
    try:
        time = time.split("-")
        if len(time) > 1:
            days = int(time[0])
            time = time[1]
        else:
            days = 0
            time = time[0]

        time = [int(e) for e in time.split(":")]

        if len(time) == 3:
            hours = time[0]
            time = time[1:]
        else:
            hours = 0

        if len(time) == 2:
            minutes = time[0]
            time = time[1:]
        else:
            minutes = 0
        seconds = time[0]
    except: #catch error where time was apparently 'INVALID'.
        days = 5 #use default full time for engine. Better than crash. 
        hours = minutes = seconds = 0


    return days * 24 * 60 * 60.0 + hours * 60 * 60.0 + minutes * 60 + seconds


queue_states = set(["PENDING", "REQUEUED"])


class Job(object):
    def __init__(
        self,
        job_name,
        jobid,
        command,
        cwd,
        env,
        ncpu,
        mem,
        reqtime,
        requeue,
        dependency,
        arch_use_add,
        arch_use_remove,
        dcache_use_add,
        dcache_use_remove,
        active_use_add,
        active_use_remove,
        partition,
        info_input_mb,
        info_output_file,
        comment,
        ssd_use='no',
        ssd_gb=0,
        owner=None
    ):
        self.job_name = job_name
        self.jobid = jobid
        self.command = command
        self.cwd = cwd
        self.env = env
        self.ncpu = ncpu
        self.mem = mem
        self.reqtime = reqtime
        self.requeue = requeue
        self.dependency = dependency
        self.archive_start_use_add = arch_use_add
        self.archive_end_use_remove = arch_use_remove
        self.dcache_start_use_add = dcache_use_add
        self.dcache_end_use_remove = dcache_use_remove
        self.active_start_use_add = active_use_add
        self.active_end_use_remove = active_use_remove
        self.partition = partition
        self.input_mb = info_input_mb
        self.output_file = info_output_file
        self.comment = comment
        self.owner = owner
        try:
            su = (ssd_use or 'no')
            if isinstance(su, str):
                su = su.strip().lower()
            else:
                su = 'no'
            if su in ('yes', 'possible', 'can_use', 'can-use', 'canuse'):
                su = 'possible'
            elif su in ('required', 'require', 'must', 'need'):
                su = 'required'
            elif su in ('no', 'none', 'off', 'false'):
                su = 'no'
            else:
                su = 'no'
        except Exception:
            su = 'no'
        self.ssd_use = su
        try:
            self.ssd_gb = max(0.0, float(ssd_gb))
        except Exception:
            self.ssd_gb = 0.0
        self.ssd_alloc_gb = 0.0

        self.state = "PENDING"
        self.node_id = None
        self.starttime = None
        self.endtime = None
        self.current_cpu_usage = 0.0
        self.current_mem_usage = 0.0
        self.submit_ts = time.time()

    def started(self, nodeid):
        self.node_id = nodeid
        self.starttime = time.time()
        self.state = "RUNNING"
        engines.job_running(nodeid, self)

    def assigned(self, nodeid):
        self.node_id = nodeid
        self.state = "ASSIGNED"

    def done(self, return_code, reason, state):
        self.endtime = time.time()
        self.state = state
        self.reason = reason
        self.return_code = return_code
        if self.node_id:
            engines.job_stopped(self.node_id, self)
        self.node_id = None


class DummyJob(Job):
    def __init__(self, jobid):
        super(DummyJob, self).__init__(
            "Dummy",
            jobid,
            "UNK",
            "UNK",
            {},
            0,
            0,
            0,
            0,
            "",
            0,
            0,
            0,
            0,
            0,
            0,
            "UNK",
            0,
            "",
            "Dummy job",
            owner=None
        )
        self.starttime = time.time()


class JobManager(object):
    def __init__(self):
        self.jobid_counter = numpy.random.randint(0, 1000000)

        self.jobs_by_id = OrderedDict()
        
        self.finished_jobs_by_owner = {}

        #self.finished_jobs = deque()
        self.finished_jobs_lock = threading.RLock()
        self.finished_jobs_max = 2000

        self.lock = threading.RLock()

        self.done_jobs = {}
        self.failed_jobs = {}  # count per partition (compute, archive)
        self.running_jobs = {}  # count per partition (compute, archive)
        self.total_jobs = {}  # count per partition (compute, archive)

        self.eligible_jobs = {}
        self._eligible_cache_ts = 0.0
        self._eligible_cache_ttl = 15.0

        self.active_total = 0
        self.active_inuse = 0
        self.archive_total = 0
        self.archive_inuse = 0
        self.dcache_total = 0
        self.dcache_inuse = 0

    def _resource_fits_locked(self, total, inuse, needed):
        try:
            needed_val = float(needed)
        except Exception:
            needed_val = 0.0
        if needed_val <= 0.0:
            return True
        try:
            total_val = float(total)
        except Exception:
            total_val = 0.0
        try:
            inuse_val = float(inuse)
        except Exception:
            inuse_val = 0.0
        return total_val >= (inuse_val + needed_val)

    def _is_job_eligible_locked(self, job):
        st = getattr(job, 'state', None)
        if st == 'ASSIGNED':
            return True
        if st not in queue_states:
            return False
        if not self._resource_fits_locked(self.active_total, self.active_inuse, getattr(job, 'active_start_use_add', 0.0)):
            return False
        if not self._resource_fits_locked(self.dcache_total, self.dcache_inuse, getattr(job, 'dcache_start_use_add', 0.0)):
            return False
        if not self._resource_fits_locked(self.archive_total, self.archive_inuse, getattr(job, 'archive_start_use_add', 0.0)):
            return False
        return True

    def _recompute_eligible_counts_locked(self):
        now = time.time()
        if (now - self._eligible_cache_ts) < self._eligible_cache_ttl:
            return

        counts = {}
        for job in list(self.jobs_by_id.values()):
            if self._is_job_eligible_locked(job):
                part = str(getattr(job, 'partition', 'compute') or 'compute')
                counts[part] = counts.get(part, 0) + 1

        self.eligible_jobs = counts
        self._eligible_cache_ts = now

    def get_job_stats(self):
        self.lock.acquire()
        try:
            self._recompute_eligible_counts_locked()
            return (
                self.total_jobs,
                self.running_jobs,
                self.eligible_jobs,
                self.failed_jobs,
                self.done_jobs,
            )
        finally:
            self.lock.release()

    def active_engines(self):
        nodes = set()
        for job in list(self.jobs_by_id.values()):
            if not job.node_id is None:
                nodes.add(job.node_id)
        return nodes

    def prioritize(self, job_pattern):
        self._reorder_jobs(job_pattern, not status.lastin_first)

    def deprioritize(self, job_pattern):
        self._reorder_jobs(job_pattern, status.lastin_first)

    def _reorder_jobs(self, job_pattern, up=True):
        self.lock.acquire()
        try:
            ndict = OrderedDict()
            if up:
                for jobid, job in list(self.jobs_by_id.items()):
                    if job_pattern in job.job_name:
                        ndict[jobid] = job
                for jobid, job in list(self.jobs_by_id.items()):
                    if not job_pattern in job.job_name:
                        ndict[jobid] = job
            else:
                for jobid, job in list(self.jobs_by_id.items()):
                    if not job_pattern in job.job_name:
                        ndict[jobid] = job
                for jobid, job in list(self.jobs_by_id.items()):
                    if job_pattern in job.job_name:
                        ndict[jobid] = job

            self.jobs_by_id = ndict
        finally:
            self.lock.release()

    def submit_job(
        self,
        job_name,
        cmd,
        cwd,
        env,
        ncpu,
        mem,
        reqtime,
        requeue,
        dependency,
        arch_use_add,
        arch_use_remove,
        dcache_use_add,
        dcache_use_remove,
        active_use_add,
        active_use_remove,
        partition,
        info_input_mb,
        info_output_file,
        comment="",
        ssd_use='no',
        ssd_gb=0,
        owner=None
    ):
        self.lock.acquire()
        jobid = str(self.jobid_counter)
        self.jobid_counter += 1
        self.total_jobs[partition] = self.total_jobs.get(partition, 0) + 1
        self.lock.release()

        # weird xml-rpc problem?
        if len(env) == 1 and "data" in env:
            env = env["data"]

        job = Job(
            job_name,
            jobid,
            cmd,
            cwd,
            env,
            ncpu,
            mem,
            reqtime,
            requeue,
            dependency,
            float(arch_use_add),
            float(arch_use_remove),
            float(dcache_use_add),
            float(dcache_use_remove),
            float(active_use_add),
            float(active_use_remove),
            partition,
            info_input_mb,
            info_output_file,
            comment,
            ssd_use,
            ssd_gb,
            owner
        )

        self.jobs_by_id[jobid] = job

        if (
            arch_use_add
            or arch_use_remove
            or dcache_use_add
            or dcache_use_remove
            or active_use_add
            or active_use_remove
        ):
            add_log_line(
                gb,
                f"Job {job_name} (id: {jobid}, ncore: {ncpu}, mem: {mem} mb, partition: {partition}, arch+: {arch_use_add} GB, arch-: {arch_use_remove} GB, d+: {dcache_use_add} GB, d-: {dcache_use_remove} GB, active+: {active_use_add} GB, active-: {active_use_remove} GB) submitted.",
                 GREEN,
            )
        else:
            add_log_line(
                gb,
                f"Job {job_name} (id: {jobid}, ncore: {ncpu}, mem: {mem} mb, partition: {partition}) submitted.",
                GREEN,
            )
        return jobid

    def cancel_job(self, jobid, requeue=False):
        self.lock.acquire()
        try:
            job = None
            if jobid in self.jobs_by_id:
                job = self.jobs_by_id[jobid]
                if job.state == "RUNNING":
                    node_id = job.node_id
                    if requeue:
                        job.requeue = max(job.requeue, 1)
                        job.reason = "Cancelling for restart"
                    else:
                        job.requeue = 0  # prevent restart
                        job.reason = "Awaiting cancellation at worker node"

                    try:
                        engines.send_command(node_id, zslurm_shared.CANCEL, jobid)
                    except:
                        pass

                    add_log_line(
                        gb,
                        "Job %s (id: %s, running on %s) has been send a cancel signal."
                        % (job.job_name, job.jobid, job.node_id),
                        YELLOW,
                    )
                    # now wait for cancel to propagate
                elif job.state == "ASSIGNED" and not requeue:
                    node_id = job.node_id
                    try:
                        engines.send_command(node_id, zslurm_shared.DEASSIGN, jobid)
                    except:
                        pass
                    self.job_done(job.jobid, RC_CANCELLED, "Cancelled by user")
                    job.state = "CANCELLED"
                    
                elif not requeue:
                    self.job_done(job.jobid, RC_CANCELLED, "Cancelled by user")
                    add_log_line(
                        gb,
                        "Job %s (id: %s) cancelled." % (job.job_name, job.jobid),
                        YELLOW,
                    )
                    job.state = "CANCELLED"
                    
            else:
                add_log_line(
                    gb, "Attempt to cancel unknown job (id: %s)." % jobid, YELLOW
                )
        finally:
            self.lock.release()

    def list_done_jobs(self, last_seen_jobid=None, owner=None):
        def serialize(job):
            if not job.starttime is None:
                runtime = job.endtime - job.starttime
            else:
                runtime = 0.0

            arch_use = job.archive_start_use_add - job.archive_end_use_remove
            active_use = job.active_start_use_add - job.active_end_use_remove
            dcache_use = job.dcache_start_use_add - job.dcache_end_use_remove

            return [
                job.jobid,
                job.job_name,
                job.state,
                runtime,
                job.ncpu,
                job.partition,
                job.node_id,
                job.current_cpu_usage,
                job.current_mem_usage,
                arch_use,
                active_use,
                dcache_use,
                job.cwd,
                job.comment,
            ]

        jobs = []
        target_jobid = None if last_seen_jobid is None else str(last_seen_jobid)
        target_owner = None if owner is None else str(owner)

        if owner is not None:
            jobs = self.finished_jobs_by_owner.get(target_owner, [])
        else:
            jobs = itertools.chain(*self.finished_jobs_by_owner.values())
        

        results = []
        if target_jobid is not None and target_owner is not None:
            for job in reversed(jobs):            
                if str(job.jobid) == target_jobid:            
                    break           
                results.append(serialize(job))
        else:
            results = [serialize(job) for job in reversed(jobs)]

        return results

    def queue_stats(self):
        # Aggregate cores/mem by status and partition
        totals = {
            'running': {'jobs': 0, 'cores': 0.0, 'mem_mb': 0.0},
            'pending': {'jobs': 0, 'cores': 0.0, 'mem_mb': 0.0},
        }
        by_part = {}
        states = {}


        for j in list(self.jobs_by_id.values()):
            part = str(getattr(j, 'partition', 'compute') or 'compute')
            st = str(getattr(j, 'state', ''))
            states[st] = states.get(st, 0) + 1
            try:
                cores = float(getattr(j, 'ncpu', 0.0))
            except Exception:
                cores = 0.0
            try:
                mem_mb = float(getattr(j, 'mem', 0.0))
            except Exception:
                mem_mb = 0.0
            if part not in by_part:
                by_part[part] = {
                    'running': {'jobs': 0, 'cores': 0.0, 'mem_mb': 0.0},
                    'pending': {'jobs': 0, 'cores': 0.0, 'mem_mb': 0.0},
                }
            is_pending = (st in queue_states) or (st == 'ASSIGNED')
            if st == 'RUNNING':
                buckets = (totals['running'], by_part[part]['running'])
            elif is_pending:
                buckets = (totals['pending'], by_part[part]['pending'])
            else:
                buckets = None
            if buckets is not None:
                for b in buckets:
                    b['jobs'] += 1
                    b['cores'] += cores
                    b['mem_mb'] += mem_mb

        qb = getattr(engines, 'cluster_queued_by_partition', {}) or {}
        rb = getattr(engines, 'cluster_running_by_partition', {}) or {}
        qa = getattr(engines, 'cluster_queued_age_by_partition', {}) or {}
        qa_s = getattr(engines, 'cluster_queued_age_by_partition_by_scratch', {}) or {}
        engines_info = {
            'running_by_partition': dict(rb),
            'queued_by_partition': dict(qb),
            'queued_age_by_partition': dict(qa),
            'queued_age_by_partition_by_scratch': dict(qa_s),
            'running_total': int(getattr(engines, 'cluster_running_engines', 0)),
            'queued_total': int(getattr(engines, 'cluster_queued_engines', 0)),
        }
        return {
            'total': totals,
            'by_partition': by_part,
            'states': states,
            'engines': engines_info,
        }

    def list_jobs(self, owner=None):
        jobs = []
        for job in list(self.jobs_by_id.values()):
            if owner is not None and str(getattr(job, 'owner', None)) != str(owner):
                continue
            if not job.starttime is None:
                runtime = time.time() - job.starttime
            else:
                runtime = 0.0

            if job.node_id is None:
                node_id = "(Resources)"
            else:
                node_id = job.node_id
            arch_use = job.archive_start_use_add - job.archive_end_use_remove
            active_use = job.active_start_use_add - job.active_end_use_remove
            dcache_use = job.dcache_start_use_add - job.dcache_end_use_remove

            jobs.append(
                [
                    job.jobid,
                    job.job_name,
                    job.state,
                    runtime,
                    job.ncpu,
                    job.partition,
                    node_id,
                    job.current_cpu_usage,
                    job.current_mem_usage,
                    arch_use,
                    active_use,
                    dcache_use,
                    job.cwd,
                    job.comment
                ]
            )

        return jobs

    def request_jobs(self, myid, current_cpu, current_mem, partition):
        if not myid in engines.engine_by_id or engines.engine_by_id[myid] is None:
            timeleft = 5 * 24 * 60 * 60
            cores = 24.0
            totmem = 64000
        else:
            engine = engines.engine_by_id[myid]
            if engine.status == PHASING_OUT:
                return []
            # Do not start or assign work to unmanaged cluster engines (seen as missing in squeue)
            if engine.cluster_id not in (None, False) and not engine.managed:
                add_log_line(
                    gb,
                    f"Node {myid} is currently unmanaged (cluster id {engine.cluster_id} not observed). No jobs will be assigned/started until managed again.",
                    YELLOW,
                )
                return []
            # Do not schedule work for engines that are stopping (teardown in progress)
            if engine.stopping:
                add_log_line(
                    gb,
                    f"Node {myid} is stopping. No jobs will be assigned/started until teardown completes.",
                    YELLOW,
                )
                return []
            timeleft = int(engines.engine_by_id[myid].timeleft)
            cores = float(engines.engine_by_id[myid].cores)
            totmem = engines.engine_by_id[myid].totmem
        e = engines.engine_by_id.get(myid, None)

        self.lock.acquire()
        try:
            queued = []
            assigned = []

            jobs_view = self.jobs_by_id.values()

            if status.lastin_first and partition != 'archive':
                jobs_view = reversed(jobs_view)
            
            jobs_view_iter = iter(jobs_view)

            allres = []
            average_mem_core = float(totmem) / float(cores)

            def _to_float(value, default=0.0):
                try:
                    if value is None:
                        return default
                    return float(value)
                except Exception:
                    return default

            def _ssd_free_gb(engine):
                if engine is None:
                    return 0.0
                total = _to_float(getattr(engine, 'ssd_total_gb', 0.0), 0.0)
                reserved = _to_float(getattr(engine, 'res_ssd_reserved_gb', 0.0), 0.0)
                return max(0.0, total - reserved)

            def _ssd_ok(job, engine):
                su = str(getattr(job, 'ssd_use', 'no') or 'no')
                need = max(0.0, _to_float(getattr(job, 'ssd_gb', 0.0), 0.0))
                has_ssd = bool(engine is not None and getattr(engine, 'has_ssd', False))
                if su == 'no':
                    return True
                if su == 'required':
                    if not has_ssd:
                        return False
                    if need > 0.0 and _ssd_free_gb(engine) < need:
                        return False
                    return True
                if su == 'possible':
                    if not has_ssd:
                        return True
                    if need > 0.0 and _ssd_free_gb(engine) < need:
                        return False
                    return True
                return True

            def eligible(job):
                if job.partition != partition:
                    return False
                if job.reqtime > timeleft:
                    return False
                if job.ncpu > cores:
                    return False
                if job.mem > totmem:
                    return False
                if not _ssd_ok(job, e):
                    return False
                if job.active_start_use_add > 0 and (self.active_inuse + job.active_start_use_add) > self.active_total:
                    return False
                if job.dcache_start_use_add > 0 and (self.dcache_inuse + job.dcache_start_use_add) > self.dcache_total:
                    return False
                if job.archive_start_use_add > 0 and (self.archive_inuse + job.archive_start_use_add) > self.archive_total:
                    return False
                return True


            while len(queued) < (status.prio_fillmem_context + float(current_cpu)):
                try: 
                    next_job = next(jobs_view_iter)
                except StopIteration:
                    break
                if not eligible(next_job):
                    continue                
                if next_job.state in queue_states:
                    queued.append(next_job)
                elif next_job.state == 'ASSIGNED':
                    assigned.append(next_job)

            while queued:
                # prioritize jobs with a good memory profile (reorder jobs)
                if status.prio_fillmem_context > 1:
                    # Greedy best-of-window selection without sorting or rebuilding the list
                    startpos = 0
                    best_idx = None
                    best_score = None
                    found = 0

                    while startpos < len(queued) and found < status.prio_fillmem_context:
                        job_candidate = queued[startpos]
                        # can the job run? recheck as constraints may have changed due to other jobs being assigned
                        if ((
                                (
                                    self.active_total
                                    >= (self.active_inuse + job_candidate.active_start_use_add)
                                )
                                or job_candidate.active_start_use_add == 0
                            )
                            and (
                                (
                                    self.dcache_total
                                    >= (self.dcache_inuse + job_candidate.dcache_start_use_add)
                                )
                                or job_candidate.dcache_start_use_add == 0
                            )
                            and (
                                (
                                    self.archive_total
                                    >= (self.archive_inuse + job_candidate.archive_start_use_add)
                                )
                                or job_candidate.archive_start_use_add == 0
                            )
                        ) and _ssd_ok(job_candidate, e):
                            remain_cores = (
                                current_cpu - job_candidate.ncpu
                            )  # remain cores after scheduling this job
                            remain_mem = (
                                current_mem - job_candidate.mem
                            )  # remain mem after scheduling this job

                            over_use_penalty = -(
                                min(remain_cores, 0)
                                + (
                                    min(remain_mem / float(average_mem_core), 0)
                                    / 1024.0
                                )
                            )  # penalty for over using cores (equal to n cores not yet available) or over using mem (too much used, in gb).
                            memory_penalty = abs((max(remain_mem, 0) / max(remain_cores, 1)) - average_mem_core) / 1024.0
                            # penalty for having memory/core > average after scheduling (in gb)
                            ssd_bonus = 0.0
                            try:
                                if job_candidate.ssd_use == 'possible' and e is not None and getattr(e, 'has_ssd', False):
                                    cap = _ssd_free_gb(e)
                                    need = float(getattr(job_candidate, 'ssd_gb', 0.0))
                                    if need <= 0.0 or cap >= need:
                                        ssd_bonus = float(getattr(status, 'ssd_prefer_bonus', 0.25))
                            except Exception:
                                import traceback
                                traceback.print_exc()
                                ssd_bonus = 0.0

                            score = memory_penalty + over_use_penalty - ssd_bonus
                            if (best_score is None) or (score < best_score):
                                best_score = score
                                best_idx = startpos
                            found += 1
                        startpos += 1

                    if best_idx is not None:
                        job = queued.pop(best_idx)
                    else:
                        # No eligible found in the window; default to pop fallback
                        fallback_idx = min(startpos, len(queued) - 1)
                        job = queued.pop(fallback_idx)
                        
                else:                    
                    job = queued.pop(0)
                # add_log_line(gb, str((job.state, job.reqtime, timeleft, job.ncpu, cores, job.mem, totmem, job.partition, partition)))
                # add_log_line(gb, str((self.active_total, self.active_inuse, job.active_start_use_add)))
                if ((
                        (
                            self.active_total
                            >= (self.active_inuse + job.active_start_use_add)
                        )
                        or job.active_start_use_add == 0
                    )
                    and (
                        (
                            self.dcache_total
                            >= (self.dcache_inuse + job.dcache_start_use_add)
                        )
                        or job.dcache_start_use_add == 0
                    )
                    and (
                        (
                            self.archive_total
                            >= (self.archive_inuse + job.archive_start_use_add)
                        )
                        or job.archive_start_use_add == 0
                    )
                    and _ssd_ok(job, e)
                ):

                    self.active_inuse += job.active_start_use_add
                    self.dcache_inuse += job.dcache_start_use_add
                    self.archive_inuse += job.archive_start_use_add

                    if not (job.ncpu <= current_cpu and job.mem <= current_mem):
                        add_log_line(
                            gb,
                            f"Job {job.job_name} (id: {job.jobid}) assigned to node {myid} ({current_cpu - job.ncpu:.1f}/{current_mem - job.mem:.2f} remaining)",
                            BLUE,
                        )
                        job.assigned(myid)
                    else:
                        add_log_line(
                            gb,
                            f"Job {job.job_name} (id: {job.jobid}) started on node {myid} ({current_cpu - job.ncpu:.1f}/{current_mem - job.mem:.2f} remaining)",
                            BLUE,
                        )
                        job.started(myid)
                        self.running_jobs[job.partition] = (
                            self.running_jobs.get(job.partition, 0) + 1
                        )

                    current_cpu -= job.ncpu
                    current_mem -= job.mem

                    if job.state == "RUNNING": #set by job.started
                        e = engines.engine_by_id.get(myid, None)
                        if e is not None:
                            e.res_cpu_reserved += float(job.ncpu)
                            e.res_mem_reserved_mb += float(job.mem)
                            try:
                                if getattr(job, 'ssd_use', 'no') != 'no' and getattr(e, 'has_ssd', False):
                                    need = float(getattr(job, 'ssd_gb', 0.0))
                                    if need > 0.0:
                                        cap = max(0.0, float(getattr(e, 'ssd_total_gb', 0.0)) - float(getattr(e, 'res_ssd_reserved_gb', 0.0)))
                                        if cap >= need:
                                            e.res_ssd_reserved_gb += need
                                            job.ssd_alloc_gb = need
                            except Exception:
                                import traceback
                                traceback.print_exc()

                    res = (
                        job.jobid,
                        job.job_name,
                        job.command,
                        job.cwd,
                        job.env,
                        job.ncpu,
                        job.mem,
                        job.state,
                    )
                    allres.append(res)
                    if current_cpu <= 0 or current_mem <= 0:
                        break
                else:
                    #rescan queued jobs instead of popping one by one
                    new_queued = []
                    for job in queued:
                        if (job.active_start_use_add > 0 and (self.active_inuse + job.active_start_use_add) > self.active_total):
                            continue
                        if (job.dcache_start_use_add > 0 and (self.dcache_inuse + job.dcache_start_use_add) > self.dcache_total):
                            continue
                        if (job.archive_start_use_add > 0 and (self.archive_inuse + job.archive_start_use_add) > self.archive_total):
                            continue
                        if not _ssd_ok(job, e):
                            continue
                        new_queued.append(job)
                    queued = new_queued

                #refill queued
                while len(queued) < (status.prio_fillmem_context + float(current_cpu)):
                    try: 
                        next_job = next(jobs_view_iter)
                    except StopIteration:
                        break
                    if not eligible(next_job):
                        continue                    
                    if next_job.state in queue_states:
                        queued.append(next_job)
                    elif next_job.state == 'ASSIGNED':
                        assigned.append(next_job)

                    

            if current_cpu > 0:                                
                for job in assigned:
                    # FIXME; assume 5 days of time                    
                    if (                                                
                        job.ncpu <= current_cpu
                        and job.mem <= current_mem  
                        and _ssd_ok(job, e)
                    ):
                        # Ask the previous engine to drop assignment and reassign to this engine.
                        try:
                            engines.send_command(
                                job.node_id, zslurm_shared.DEASSIGN, job.jobid
                            )
                        except KeyError:
                            pass

                        # Reassign but do not start here; require handshake via can_run_assigned_job
                        job.assigned(myid)
                        add_log_line(
                            gb,
                            f"Job {job.job_name} (id: {job.jobid}) reassigned to node {myid} (ASSIGNED; will start after handshake) ({current_cpu}/{current_mem} remaining)",
                            BLUE,
                        )

                        current_cpu -= job.ncpu
                        current_mem -= job.mem

                        res = (
                            job.jobid,
                            job.job_name,
                            job.command,
                            job.cwd,
                            job.env,
                            job.ncpu,
                            job.mem,
                            job.state,
                        )
                        allres.append(res)
                        if current_cpu <= 0 or current_mem <= 0:
                            break
        except Exception as e:
            import traceback

            traceback.print_exc()
            raise
        finally:
            self.lock.release()
        return allres

    def engine_removed(self, engine_id, reason="Engine disappeared"):
        self.lock.acquire()
        try:
            for job in list(self.jobs_by_id.values()):
                if job.node_id == engine_id:
                    if job.state == "RUNNING":
                        self.job_done(job.jobid, RC_ENGINE_TERMINATED, reason=reason)
                    elif job.state == "ASSIGNED":
                        job.state = "PENDING"
        finally:
            self.lock.release()

    def can_run_assigned_job(self, nodeid, jobid):
        self.lock.acquire()
        try:
            res = False  # DROP
            if jobid in self.jobs_by_id:
                job = self.jobs_by_id[jobid]
                # Only the engine currently assigned to this job may start it
                engine = engines.engine_by_id.get(nodeid, None)
                if engine and engine.stopping:
                    # Do not allow starts on engines that are stopping
                    res = False
                elif job.state == "ASSIGNED" and job.node_id == nodeid:
                    su = str(getattr(job, 'ssd_use', 'no') or 'no')
                    try:
                        need = max(0.0, float(getattr(job, 'ssd_gb', 0.0) or 0.0))
                    except Exception:
                        need = 0.0
                    has_ssd = bool(engine is not None and getattr(engine, 'has_ssd', False))
                    free_ssd = 0.0
                    if engine is not None:
                        try:
                            free_ssd = max(0.0, float(getattr(engine, 'ssd_total_gb', 0.0)) - float(getattr(engine, 'res_ssd_reserved_gb', 0.0)))
                        except Exception:
                            free_ssd = 0.0
                    ssd_ok = True
                    if su == 'required':
                        if not has_ssd:
                            ssd_ok = False
                        elif need > 0.0 and free_ssd < need:
                            ssd_ok = False
                    elif su == 'possible':
                        if has_ssd and need > 0.0 and free_ssd < need:
                            ssd_ok = False

                    if ssd_ok:
                        self.job_start(nodeid, jobid)
                        res = True
                    else:
                        try:
                            # Release resource reservation from ASSIGNED state
                            self.active_inuse -= float(getattr(job, 'active_start_use_add', 0.0) or 0.0)
                            self.dcache_inuse -= float(getattr(job, 'dcache_start_use_add', 0.0) or 0.0)
                            self.archive_inuse -= float(getattr(job, 'archive_start_use_add', 0.0) or 0.0)
                            if self.active_inuse < 0:
                                self.active_inuse = 0
                            if self.dcache_inuse < 0:
                                self.dcache_inuse = 0
                            if self.archive_inuse < 0:
                                self.archive_inuse = 0
                        except Exception:
                            pass
                        job.node_id = None
                        job.state = "PENDING"
        finally:
            self.lock.release()
        return res

    def job_start(self, node_id, jobid):
        self.lock.acquire()
        try:
            job = self.jobs_by_id[jobid]
            self.running_jobs[job.partition] = (
                self.running_jobs.get(job.partition, 0) + 1
            )
            job.started(node_id)
            e = engines.engine_by_id.get(node_id, None)
            if e is not None:
                e.res_cpu_reserved += float(job.ncpu)
                e.res_mem_reserved_mb += float(job.mem)
                try:
                    if getattr(job, 'ssd_use', 'no') != 'no' and getattr(e, 'has_ssd', False):
                        need = float(getattr(job, 'ssd_gb', 0.0))
                        if need > 0.0:
                            cap = max(0.0, float(getattr(e, 'ssd_total_gb', 0.0)) - float(getattr(e, 'res_ssd_reserved_gb', 0.0)))
                            if cap >= need:
                                e.res_ssd_reserved_gb += need
                                job.ssd_alloc_gb = need
                except Exception:
                    import traceback
                    traceback.print_exc()
        finally:
            self.lock.release()

    def job_done(self, jobid, return_code, reason="No Reason"):
        # FIXME: add in requeue stuff
        self.lock.acquire()
        try:
            job = self.jobs_by_id.get(jobid, None)
            if not job is None:
                if not job.node_id is None:
                    if return_code == 0:
                        self.active_inuse -= job.active_end_use_remove
                        self.archive_inuse -= job.archive_end_use_remove
                        self.dcache_inuse -= job.dcache_end_use_remove
                    else:  # failed job, assume space is free again
                        self.active_inuse -= job.active_start_use_add
                        self.archive_inuse -= job.archive_start_use_add
                        self.dcache_inuse -= job.dcache_start_use_add
                    e = engines.engine_by_id.get(job.node_id, None)
                    if e is not None:
                        e.res_cpu_reserved -= float(job.ncpu)
                        e.res_mem_reserved_mb -= float(job.mem)
                        try:
                            alloc = float(getattr(job, 'ssd_alloc_gb', 0.0))
                            if alloc > 0.0:
                                e.res_ssd_reserved_gb -= alloc
                                if e.res_ssd_reserved_gb < 0:
                                    e.res_ssd_reserved_gb = 0.0
                            job.ssd_alloc_gb = 0.0
                        except Exception:
                            import traceback
                            traceback.print_exc()
                        if e.res_cpu_reserved < 0:
                            e.res_cpu_reserved = 0.0
                        if e.res_mem_reserved_mb < 0:
                            e.res_mem_reserved_mb = 0.0

                if (
                    job.state == "RUNNING"
                    and job.node_id is not None
                    and return_code != RC_CANCELLED
                ):
                    self.running_jobs[job.partition] = max(
                        0, self.running_jobs.get(job.partition, 0) - 1
                    )

                if (
                    return_code != RC_SUCCESS
                    and return_code != RC_TERMINATED
                    and job.requeue > 0
                ):
                    job.requeue -= 1
                    job.mem = job.mem * 2
                    newstate = "REQUEUED"
                    job.starttime = None
                    job.node_id = None
                elif (
                    return_code == RC_ENGINE_TERMINATED
                ):  # ENGINE DISAPPEARED/TERMINATED
                    newstate = "REQUEUED"
                    job.starttime = None
                    job.node_id = None
                else:
                    job = self.jobs_by_id[jobid]
                    
                    del self.jobs_by_id[jobid]
                    self.done_jobs[job.partition] = (
                        self.done_jobs.get(job.partition, 0) + 1
                    )
                    self.total_jobs[job.partition] = (
                        self.total_jobs.get(job.partition, 0) - 1
                    )
                    if (
                        return_code != RC_SUCCESS
                        and return_code != RC_CANCELLED
                        and return_code != RC_TERMINATED
                    ):
                        self.failed_jobs[job.partition] = (
                            self.failed_jobs.get(job.partition, 0) + 1
                        )
                        newstate = "FAILED"
                    elif return_code == RC_CANCELLED or return_code == RC_TERMINATED:
                        newstate = "CANCELLED"
                    else:
                        newstate = "COMPLETED"

                job.done(return_code, reason, newstate)
                with self.finished_jobs_lock:
                    if job.owner not in self.finished_jobs_by_owner:
                        self.finished_jobs_by_owner[job.owner] = []
                    
                    self.finished_jobs_by_owner[job.owner].append(job)
                    
                    if len(self.finished_jobs_by_owner[job.owner]) > (self.finished_jobs_max + 100):
                        self.finished_jobs_by_owner[job.owner] = self.finished_jobs_by_owner[job.owner][-self.finished_jobs_max:]                    
                        
                

        finally:
            self.lock.release()
        if not job is None:
            return job.job_name
        else:
            return ""

    def job_finished(self, myid, jobid, return_code, report):
        if return_code == 0:
            reason = "Job Finished"
        elif return_code == 143 or return_code == 15:
            reason = "Job Terminated"
        elif return_code == 137:
            reason = "Job Killed"
        elif return_code == 130:
            reason = "Job Interrupted"
        elif return_code == -254:
            reason = "Job Stuck (terminated by monitor)"
        else:
            reason = "Job Failed (errorcode: %d)" % return_code

        name = "unknown"
        comment = ""
        self.lock.acquire()
        core_reserved = 0
        mem_reserved = 0
        input_mb = 0
        output_file = ""
        try:
            if not jobid in self.jobs_by_id:
                add_log_line(
                    gb,
                    "Unknown job (id: %s) completed at %s (%s)" % (jobid, myid, reason),
                    YELLOW,
                )
                #insert a Dummy Job with the correct id so that we correctly report back to snakemake
                job = DummyJob(jobid)
                state = 'COMPLETED' if return_code == 0 else 'FAILED'
                job.done(return_code, reason, state)

            else:
                job = self.jobs_by_id[jobid]
                comment = job.comment
                mem_reserved = job.mem
                core_reserved = job.ncpu
                input_mb = job.input_mb
                output_file = job.output_file
                name = self.job_done(jobid, return_code, reason)

        finally:
            self.lock.release()

        if "starttime" in report:
            line = [
                name,
                comment,
                report.get("retcode", ""),
                myid,
                jobid,
                report.get("starttime", ""),
                report.get("endtime", ""),
                core_reserved,
                mem_reserved,
                input_mb,
                output_file,
            ]
            nostat = [""] * 7
            for lab in ["uss", "rss", "vms", "cpu_percentage"]:
                line.extend(report.get(lab, nostat))

            for lab in [
                "runtime",
                "montime",
                "avg_cpu_percentage",
                "nthreads_avg",
                "nthreads_max",
                "user",
                "system",
                "maxrss",
                "mon_user",
                "mon_system",
                "iowait",
                "read_count",
                "write_count",
                "read_bytes",
                "write_bytes",
            ]:
                line.append(report.get(lab, ""))
            line.append(report.get('memory_over_time',''))
            status.reports_file.write("\t".join([str(e) for e in line]) + "\n")
            status.reports_file.flush()

        if return_code == 0:
            runtime = zslurm_shared.format_time(report.get("runtime", 0.0))
            add_log_line(
                gb,
                "Job %s (id: %s) completed after %s." % (name, jobid, runtime),
                GREEN,
            )
        else:
            add_log_line(
                gb,
                "Job %s (id: %s, node: %s) failed, reason: %s."
                % (name, jobid, myid, reason),
                YELLOW,
            )


jobs = JobManager()
submit_job = lambda *args: jobs.submit_job(*args)
cancel_job = lambda *args: jobs.cancel_job(*args)
list_jobs = lambda *args: jobs.list_jobs(*args)
list_done_jobs = lambda *args: jobs.list_done_jobs(*args)
queue_stats = lambda *args: jobs.queue_stats(*args)
request_jobs = lambda *args: jobs.request_jobs(*args)
job_finished = lambda *args: jobs.job_finished(*args)
can_run_assigned_job = lambda *args: jobs.can_run_assigned_job(*args)


class RequestHandler(SimpleXMLRPCRequestHandler):
    def __init__(self, rpc_path):
        rpc_paths = (f"/{rpc_path}",)

def thread_start_job_server(port, rpc_path):
    request_handeler=SimpleXMLRPCRequestHandler
    request_handeler.rpc_paths=(f"/{rpc_path}",)
    with SimpleXMLRPCServer(
        ("", int(port)),
        logRequests=False,
        allow_none=True,
        requestHandler=request_handeler) as server:
        server.register_introspection_functions()

        server.register_function(submit_job, "submit_job")
        server.register_function(cancel_job, "cancel_job")
        server.register_function(list_jobs, "list_jobs")
        server.register_function(list_done_jobs, "list_done_jobs")
        server.register_function(queue_stats, "queue_stats")
        server.register_function(list_nodes, "list_nodes")
        add_log_line(gb, f"[job] started on port {port} with rpc path {rpc_path}", GREEN)
        Servers.job_server = server
        server.serve_forever()





class Engine(object):
    def __init__(
        self,
        engine_id="?",
        cores=0,
        totmem=0,
        partition="",
        cluster_id=None,
        managed=False,
        instance=None,
    ):
        self.engine_id = engine_id
        self.cluster_id = cluster_id
        self.instance = instance
        self.managed = managed
        self.partition = partition

        self.cores = cores
        self.totmem = totmem
        self.jobs = set()
        self.status = ""

        self.cpu_usage = 0.0
        self.mem_usage = 0.0
        self.load = 0.0
        self.sys_cpu_busy = 0.0
        self.sys_iowait = 0.0
        self.res_cpu_reserved = 0.0
        self.res_mem_reserved_mb = 0.0

        self.starttime = time.time()
        self.timeleft = 5 * 24 * 60 * 60
        self.lastseen = time.time()

        self.pending_commands = []
        self.observed_hits = 0
        self.missing_hits = 0
        self.stopping = False

        self.has_ssd = False
        self.ssd_total_gb = 0.0
        self.ssd_used_gb = 0.0        
        self.res_ssd_reserved_gb = 0.0


class EngineManager(object):
    def __init__(self):
        self.lock = threading.RLock()
        self.engine_by_id = {}

        self.engine_by_clusterid = {}
        self.local_engine_process = None

        self.cluster_queued_engines = 0
        self.cluster_running_engines = 0

        self.cluster_queued_engines_archive = 0
        self.cluster_running_engines_archive = 0
        self.cluster_queued_age_by_partition = {}
        self.cluster_queued_age_by_partition_by_scratch = {}
        self.last_squeue_ts = 0.0
        self.archive_failed_nodes = set()

    def toggle_phasing_out(self, engine_id):
        with self.lock:
            cmd = ("" if engine_id is None else str(engine_id)).strip()

            if not cmd:
                return "No engine specified."

            engine = self.engine_by_id.get(cmd)
            if engine is not None:
                if engine.status == PHASING_OUT:
                    engine.status = ""
                    return f"Engine {cmd} is no longer phasing out."
                engine.status = PHASING_OUT
                return f"Engine {cmd} is now phasing out."

            partition_map = {
                "archive": "archive",
                "arch": "archive",
                "a": "archive",
                "compute": "compute",
                "comp": "compute",
                "c": "compute",
            }

            partition_hint = None
            nr_str = cmd

            parts = cmd.split(None, 1)
            if len(parts) == 2 and parts[0].lower() in partition_map:
                partition_hint = partition_map[parts[0].lower()]
                nr_str = parts[1].strip()

            try:
                nr_of_engines = int(nr_str)
            except ValueError:
                known = ", ".join(sorted(self.engine_by_id.keys()))
                return f"Engine '{cmd}' not found. Known engines: {known}."

            target_partition = (partition_hint or "compute").lower()

            def matches_partition(candidate):
                part = (getattr(candidate, "partition", "") or "").lower()
                if target_partition == "archive":
                    return part == "archive"
                return part != "archive"

            if nr_of_engines == 0:
                return "No engines selected for phase change (0 requested)."

            if nr_of_engines > 0:
                candidates = [
                    e
                    for e in self.engine_by_id.values()
                    if e.status != PHASING_OUT and matches_partition(e)
                ]
                candidates.sort(key=lambda e: len(e.jobs))
                selected = candidates[: nr_of_engines]
                for item in selected:
                    item.status = PHASING_OUT
                count = len(selected)
                if count == 0:
                    return f"No {target_partition} engines available to phase out."
                names = ", ".join(e.engine_id for e in selected)
                msg = (
                    f"Phasing out {count} {target_partition}"
                    f" engine{'s' if count != 1 else ''}: {names}."
                )
                if count < nr_of_engines:
                    missing = nr_of_engines - count
                    msg += (
                        f" Skipped {missing} requested engine"
                        f"{'s' if missing != 1 else ''} due to availability."
                    )
                return msg

            desired = -nr_of_engines
            candidates = [
                e
                for e in self.engine_by_id.values()
                if e.status == PHASING_OUT and matches_partition(e)
            ]
            candidates.sort(key=lambda e: len(e.jobs), reverse=True)
            selected = candidates[:desired]
            for item in selected:
                item.status = ""
            count = len(selected)
            if count == 0:
                return f"No phased-out {target_partition} engines to restore."
            names = ", ".join(e.engine_id for e in selected)
            msg = (
                f"Dephasing {count} {target_partition}"
                f" engine{'s' if count != 1 else ''}: {names}."
            )
            if count < desired:
                missing = desired - count
                msg += (
                    f" Skipped {missing} requested engine"
                    f"{'s' if missing != 1 else ''} due to availability."
                )
            return msg

    def job_running(self, eid, job):
        engine = self.engine_by_id.get(eid, None)
        if engine:
            engine.jobs.add(job)

    def job_stopped(self, eid, job):
        engine = self.engine_by_id.get(eid, None)
        if engine:
            engine.jobs.discard(job)

    def list_nodes(self):
        result = []
        for e in list(self.engine_by_id.values()):
            result.append(
                (
                    e.partition,
                    e.engine_id,
                    e.cluster_id if e.cluster_id is not False else "NA",
                    e.cores,
                    e.totmem,
                    time.time() - e.starttime,
                    e.timeleft,
                    len(e.jobs),
                    e.cpu_usage,
                    e.mem_usage,
                    e.load,
                    e.status,
                    e.sys_cpu_busy,
                    e.sys_iowait,
                    getattr(e, 'res_cpu_reserved', 0.0),
                    getattr(e, 'res_mem_reserved_mb', 0.0),
                    bool(getattr(e, 'has_ssd', False)),
                    float(getattr(e, 'ssd_total_gb', 0.0)),
                    float(getattr(e, 'ssd_used_gb', 0.0)),
                    float(getattr(e, 'res_ssd_reserved_gb', 0.0)),                    
                )
            )
        return result

    def send_command(self, nodeid, command, param=None):
        self.lock.acquire()
        try:
            engines.engine_by_id[nodeid].pending_commands.append((command, param))
        finally:
            self.lock.release()

    def check_local_engine(self):
        if (
            not self.local_engine_process is None
            and self.local_engine_process.poll() != None
        ):
            self.lock.acquire()
            if (
                not self.local_engine_process is None
                and self.local_engine_process.poll() != None
            ):  # recheck within lock
                self.local_engine_process = None
            self.lock.release()

        return self.local_engine_process != None

    def _check_cluster_engines(self):
        cmd = "squeue -h -r --user " + getpass.getuser() + " --name " + status.instance_name + " -O arrayjobid:|,arraytaskid:|,name:|,timeleft:|,statecompact:|,nodelist:|,partition:|,pendingtime:|,feature"

        args = shlex.split(cmd)
        try:
            p = Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            output = p.communicate()[0]
            if isinstance(output, bytes):
                output = output.decode("utf-8")
        except OSError:
            # lets assume this is temporary
            return

        output = [e.strip().split("|") for e in output.split("\n") if e]

        z = defaultdict(int)

        observed_cluster_ids = set()
        ages_any = defaultdict(float)
        ages_scratch = defaultdict(float)
        ages_noscratch = defaultdict(float)
        nqueue = 0
        nqueue_archive = 0
        nrunning = 0
        nrunning_archive = 0
        queued_by = defaultdict(int)
        running_by = defaultdict(int)
        
        
        try:
            for jid_raw, atask, cname, ctime, cstate, cnode, cpartition, cpending, cfeatures in output:
                task = (atask or "").strip()
                if task and task != "N/A":
                    cid = f"{jid_raw}_{task}"
                else:
                    cid = jid_raw
        

                observed_cluster_ids.add(cid)

                if cstate == "CG":  # cleaning up
                    continue

                # indicate this engine is observed; apply gating to reduce flapping                
                if cid in self.engine_by_clusterid:
                    engine = self.engine_by_clusterid[cid]
                    engine.missing_hits = 0
                    engine.observed_hits = engine.observed_hits + 1                    
                    if (not engine.managed) and engine.observed_hits >= SQUEUE_OBSERVED_CONFIRMS:
                        engine.managed = True
                        add_log_line(
                            gb,
                            f"Engine {engine.engine_id} @ {engine.cluster_id} is managed again after {engine.observed_hits} observations.",
                            CYAN,
                        )            
                    if cstate == "R":
                        engine.timeleft = parse_timeleft(ctime)

                # Track max pending age per Slurm partition for zslurm engine jobs; split by constraint (scratch-node)
                if cstate != "R":
                    try:
                        age_sec = parse_timeleft(cpending)
                    except Exception:
                        age_sec = 0.0
                    try:
                        features_str = (cfeatures or "").strip()
                    except Exception:
                        features_str = ""
                    try:
                        ages_any[cpartition] = max(float(ages_any.get(cpartition, 0.0)), float(age_sec))
                        if "scratch-node" in features_str:
                            ages_scratch[cpartition] = max(float(ages_scratch.get(cpartition, 0.0)), float(age_sec))
                        else:
                            ages_noscratch[cpartition] = max(float(ages_noscratch.get(cpartition, 0.0)), float(age_sec))
                    except Exception:
                        pass

                if cpartition == "staging":
                    if cstate == "R":
                        # Count running engines even if not yet registered; they occupy cluster capacity
                        nrunning_archive += 1
                        running_by[cpartition] += 1
                    else:
                        nqueue_archive += 1
                        queued_by[cpartition] += 1
                else:
                    if cstate == "R":
                        # Count running engines even if not yet registered; they occupy cluster capacity
                        nrunning += 1
                        running_by[cpartition] += 1
                    else:
                        nqueue += 1
                        queued_by[cpartition] += 1



                z[cstate] += 1
        except Exception as ex:
            add_log_line(
                gb,
                f"Exception while parsing squeue output in _check_cluster_engines: {ex}",
                RED,
            )
            add_log_line(gb, traceback.format_exc(), RED)
            return

        for cid, engine in list(self.engine_by_clusterid.items()):
            if (
                not engine.cluster_id is False
                and not engine.cluster_id in observed_cluster_ids
            ):
                # Missed this cycle; increment misses and reset observations
                engine.observed_hits = 0
                engine.missing_hits = engine.missing_hits + 1
                # Only mark unmanaged after tolerance to avoid flapping
                if engine.managed and engine.missing_hits >= SQUEUE_MISSING_TOLERANCE:
                    engine.managed = False
                    add_log_line(
                        gb,
                        f"Engine {engine.engine_id} @ {engine.cluster_id} not observed in Slurm {engine.missing_hits} times; deferring unregister until TIMEOUT.",
                        YELLOW,
                    )

        self.cluster_queued_engines = nqueue
        self.cluster_running_engines = nrunning
        self.cluster_queued_engines_archive = nqueue_archive
        self.cluster_running_engines_archive = nrunning_archive
        self.last_observed_cids = set(observed_cluster_ids)
        # Per-Slurm-partition counts (e.g., genoa, rome, fat_genoa, fat_rome)
        try:
            self.cluster_queued_by_partition = dict(queued_by)
            self.cluster_running_by_partition = dict(running_by)
        except Exception:
            self.cluster_queued_by_partition = {}
            self.cluster_running_by_partition = {}
        try:
            self.cluster_queued_age_by_partition = dict(ages_any)
            keys = set(list(ages_scratch.keys()) + list(ages_noscratch.keys()))
            by_s = {}
            for p in keys:
                by_s[p] = {
                    'scratch': float(ages_scratch.get(p, 0.0)),
                    'no_scratch': float(ages_noscratch.get(p, 0.0)),
                }
            self.cluster_queued_age_by_partition_by_scratch = by_s
            self.last_squeue_ts = time.time()
        except Exception:
            pass

    def count_cluster_engines(self):
        return len(self.engine_by_clusterid)

    def start_local(self, gb, ncpu, mem):
        self.lock.acquire()
        try:
            if not self.local_engine_process is None:
                set_status_message(gb, "Local engine already started!", YELLOW)
            elif ncpu <= 0 or mem <= 0:
                set_status_message(
                    gb,
                    "Invalid cpu (%d) or mem (%.2g gb) constraint. "
                    % (ncpu, mem / 1024.0),
                    YELLOW,
                )
            else:
                set_status_message(gb, "Starting local engines", YELLOW)
                chief_path = "zslurm_chief"
                cmd = (
                    chief_path
                    + f" -a {status.address} -p {status.port} -c {ncpu} -m {mem} -u"
                )
                add_log_line(gb, "Starting local engine using: " + cmd, GREEN)
                args = shlex.split(cmd)

                e = Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                log_output(e.stdout, gb, MAGENTA)
                log_output(e.stderr, gb, RED)
                self.local_engine_process = e

                set_status_message(gb, "Local engine started", GREEN)
                add_log_line(gb, "Local engine started", MAGENTA)
        finally:
            self.lock.release()

    def stop_local(self, gb):
        if self.local_engine_process is None:
            set_status_message(gb, "No local engine running", YELLOW)
        else:
            set_status_message(gb, "Stopping local engine", YELLOW)
            stop_engine = self.local_engine_process
            if stop_engine.poll() is None:
                add_log_line(gb, "Termination signal send to local engine", MAGENTA)
                stop_engine.terminate()

            for engine in list(self.engine_by_id.values()):
                if engine.cluster_id is False:
                    self.send_command(engine.engine_id, zslurm_shared.DIE)

            counter = 65
            while counter > 0 and stop_engine.poll() is None:
                if (counter % 10) == 0:
                    add_log_line(
                        gb,
                        "Waiting %d seconds for local engine to terminate..." % counter,
                        MAGENTA,
                    )
                time.sleep(1)
                counter -= 1
            if stop_engine.poll() is None:
                stop_engine.kill()
                add_log_line(gb, "Send local engine a kill signal...", RED)

            self.local_engine_process = None
            set_status_message(gb, "Local engine terminated", GREEN)

    # FIXME
    def start_slurm(self, gb, n, stime, partition, constraint, cores):
        self.lock.acquire()
        started_engines = []
        try:
            if n < 0:
                set_status_message(
                    gb, "Invalid number of engines to be started: " + str(n), YELLOW
                )
            else:
                set_status_message(gb, "Starting SLURM engines", YELLOW)
                # Build sbatch command as a list to avoid quoting issues
                args = [
                    "sbatch", "-N", "1", "-p", str(partition),
                ]
                if cores > 0:
                    args.append(f"--cpus-per-task={int(cores)}")
                else:
                    args.append("--exclusive")
                if constraint:
                    args.extend(["--constraint", str(constraint)])
                args.append("--parsable")
                if n > 1:
                    args.append(f"--array=0-{int(n)-1}")
                args.extend(["-J", str(status.instance_name), "-t", str(stime)])

                # Partition mapping for manager side
                if partition == "staging":
                    # check for existing and failed archive nodes to exclude
                    xlist = []
                    for eid, engine in self.engine_by_id.items():
                        if engine.partition == "archive":
                            eid = eid.split('-')[0]
                            xlist.append(eid)
                    xlist.extend(list(self.archive_failed_nodes))
                    if xlist:
                        args.append("--exclude=" + ",".join(sorted(set(filter(None, xlist)))))
                    lpartition = "archive"
                else:
                    lpartition = "compute"

                # Add command to run under sbatch and its arguments
                args.extend([
                    "slurm_to_zslurm",
                    "-a", str(status.address),
                    "-p", str(status.port),
                    "-t", str(lpartition),
                ])
                if cores > 0:
                    args.extend(["-c", str(int(cores))])

                eid = Popen(
                    args, stdout=subprocess.PIPE, stderr=subprocess.PIPE
                ).communicate()[0]
                if isinstance(eid, bytes):
                    eid = eid.decode("utf-8")

                eid = eid.strip()
                try:
                    log_cmd = " ".join(shlex.quote(str(a)) for a in args)
                except Exception:
                    log_cmd = "sbatch ..."
                add_log_line(gb, "Queueing engines (%s) using: " % eid + log_cmd, GREEN)
                if "error" in eid or not eid:
                    add_log_line(gb, "Error during SLURM submission.", RED)
                    return

                if n == 1:
                    self.engine_by_clusterid[eid] = Engine(cluster_id=eid, managed=True, instance=status.instance_name)
                    started_engines.append(self.engine_by_clusterid[eid])
                else:
                    for i in range(n):
                        cid = eid + "_" + str(i)                        
                        self.engine_by_clusterid[cid] = Engine(
                            cluster_id=cid, managed=True, instance=status.instance_name
                        )
                        started_engines.append(self.engine_by_clusterid[cid])
                time.sleep(3)
                set_status_message(gb, "SLURM engines queued", GREEN)
                add_log_line(gb, str(n) + " SLURM engines queued", MAGENTA)
        finally:
            self.lock.release()
        return started_engines

    # FIXME: jobs running removal, priority of cancelling less active nodes
    def stop_slurm(self, gb, n):
        self.lock.acquire()
        try:
            if self.count_cluster_engines() == 0 and n != 0:
                set_status_message(gb, "No SLURM engines running", YELLOW)
            elif n < 0 or n > self.count_cluster_engines():
                set_status_message(
                    gb, "Invalid number of engines to be stopped: " + str(n), YELLOW
                )
            else:
                set_status_message(gb, f"Stopping {n} SLURM engines", YELLOW)

                # stop queued engines
                stop_engines = []

                for cid, engine in list(self.engine_by_clusterid.items()):
                    if engine is None:
                        stop_engines.append(cid)

                stop_engines.sort()
                stop_engines = stop_engines[::-1]  # cancel youngest non-running engine
                # stop idle engines
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in list(self.engine_by_clusterid.items()):
                        if engine is None or not engine.managed:
                            continue
                        if len(engine.jobs) == 0:
                            stop_engines2.append(cid)
                    stop_engines2.sort()  # cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2

                # stop oldest engines
                if len(stop_engines) < n:
                    stop_engines2 = []
                    for cid, engine in list(self.engine_by_clusterid.items()):
                        if engine is None or not engine.managed:
                            continue
                        if not cid in stop_engines:
                            stop_engines2.append(cid)
                    stop_engines2.sort()  # cancel oldest running engine
                    stop_engines = stop_engines + stop_engines2

                # as a last resort, include any remaining engines regardless of managed state
                if len(stop_engines) < n:
                    for cid, engine in list(self.engine_by_clusterid.items()):
                        if cid not in stop_engines:
                            stop_engines.append(cid)

                stop_engines = stop_engines[:n]
                if stop_engines:
                    add_log_line(gb, "Stop engines: %s" % str(stop_engines), CYAN)

                    set_progress_bar(gb, 0.0)
                    for pos, eid in enumerate(stop_engines):
                        self.cancel_cid(eid)
                        set_progress_bar(gb, float(pos + 1) / float(n))

                    time.sleep(2)
                    stop_progress_bar(gb)
                    set_status_message(gb, "SLURM engines stopped", GREEN)
                    add_log_line(gb, str(n) + " SLURM engines stopped", MAGENTA)
        finally:
            self.lock.release()

    def cancel_cid(self, cid, unregister_after=True):
        self.lock.acquire()
        try:
            cmd = "scancel " + str(cid)
            args = shlex.split(cmd)
            add_log_line(gb, "Cancelling using command: " + cmd, CYAN)
            p = Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            log_output(p.stdout, gb, WHITE)
            log_output(p.stderr, gb, RED)
            p.wait()

            if cid in self.engine_by_clusterid:
                e = self.engine_by_clusterid[cid]
                if not e is None:
                    # mark as stopping while Slurm cancellation propagates
                    e.stopping = True
                    if unregister_after:
                        self._unregister(e, reason="Engine stopped")
        finally:
            self.lock.release()

    def _is_cid_present(self, cid):
        """Return True if the given Slurm cluster job id is still present in squeue."""
        try:
            cids = getattr(self, 'last_observed_cids', set()) or set()
            return str(cid) in cids
        except Exception:
            return False

    def _wait_for_cid_termination(self, cid, timeout_seconds=60, poll_interval=2):
        """Wait up to timeout_seconds for the Slurm job cid to disappear from squeue."""
        if cid is None or cid is False:
            return
        start = time.time()
        while time.time() - start < timeout_seconds:
            if not self._is_cid_present(cid):
                add_log_line(gb, f"Confirmed SLURM job {cid} is no longer present.", CYAN)
                return
            time.sleep(poll_interval)
        add_log_line(gb, f"Timed out waiting for SLURM job {cid} to disappear; proceeding.", YELLOW)

    def stop_all(self):
        if self.count_cluster_engines() > 0:
            add_log_line(gb, "Terminating SLURM engines.", RED)
            self.stop_slurm(gb, self.count_cluster_engines())
        if self.check_local_engine():
            add_log_line(gb, "Terminating local engine.", RED)
            self.stop_local(gb)


    def failed_node(self, myip, partition, cluster_jobid):        
        add_log_line(gb, f"Node {myip} at {cluster_jobid} in {partition} reports a failed node, and will exit.")
        if str(partition) == "archive" and myip:
            self.lock.acquire()
            try:
                self.archive_failed_nodes.add(myip)
            finally:
                self.lock.release()

    def register(
        self,
        myip,
        cores,
        totmem,
        partition,
        cluster_jobid,
        ssd_total_gb,
        ssd_used_gb,      
    ):
        self.lock.acquire()
        try:
            myid = zslurm_shared.short_name(myip)

            myx = myid
            counter = 1
            while myx in self.engine_by_id:
                counter += 1
                myx = f"{myid}-{counter}"
            myid = myx

            prev = self.engine_by_clusterid.get(cluster_jobid, None) if cluster_jobid not in (None, "", False) else None
            if prev is not None:
                # Reuse placeholder to preserve managed/observed state
                e = prev
                e.engine_id = myid
                e.cores = cores
                e.totmem = totmem
                e.partition = partition
                e.status = ""
            else:
                e = Engine(myid, cores, totmem, partition)
                # Capture pre-existing engines as managed on registration; squeue will demote if missing later
                if cluster_jobid not in (None, "", False):
                    e.managed = True
                if cluster_jobid not in (None, "", False):
                    self.engine_by_clusterid[cluster_jobid] = e

            self.engine_by_id[myid] = e
            if myid == zslurm_shared.short_name(status.address) or cluster_jobid == "":
                e.cluster_id = False
            else:
                e.cluster_id = cluster_jobid
                self.engine_by_clusterid[cluster_jobid] = e

            e.has_ssd = False
            e.ssd_total_gb = float(ssd_total_gb)
            e.ssd_used_gb = float(ssd_used_gb)            
            e.res_ssd_reserved_gb = 0.0
            e.has_ssd = e.ssd_total_gb > 0.0

            totalgb = totmem / 1024.0
            average_use = totalgb / float(cores)

            add_log_line(
                gb,
                "Engine ("
                + str(cores)
                + "C:"
                + "{:1.1f}".format(average_use)
                + f" GB, {e.ssd_total_gb:.1f} GB SSD, {partition}) on {myid} is now online @ {cluster_jobid}",
                CYAN,
            )

        except Exception as e:
            add_log_line(gb, "Exception in register: " + str(e), RED)
        finally:
            self.lock.release()
        return myid

    def poll(
        self,
        myid,
        cpu_usage,
        mem_usage,
        load,
        mode,
        idle,
        current_cpu_usage,
        current_mem_usage,
        sys_cpu_busy,
        sys_iowait,
        ssd_total_gb,
        ssd_used_gb
    ):
        try:
            e = self.engine_by_id[myid]
            e.cpu_usage = cpu_usage
            e.mem_usage = mem_usage
            e.load = load
            try:
                e.sys_cpu_busy = float(sys_cpu_busy) if not sys_cpu_busy is None else 0.0
            except Exception:
                e.sys_cpu_busy = 0.0
            try:
                e.sys_iowait = float(sys_iowait) if not sys_iowait is None else 0.0
            except Exception:
                e.sys_iowait = 0.0

            def _to_float(value, default=None):
                if value is None:
                    return default
                try:
                    return float(value)
                except Exception:
                    return default

            total_ssd = _to_float(ssd_total_gb, None)
            if total_ssd is not None:
                total_ssd = max(0.0, total_ssd)
                e.ssd_total_gb = total_ssd
                e.has_ssd = total_ssd > 0.0
            elif getattr(e, "ssd_total_gb", None) is None:
                e.ssd_total_gb = 0.0
                e.has_ssd = False

            used_ssd = _to_float(ssd_used_gb, None)
            if used_ssd is not None:
                used_ssd = max(0.0, used_ssd)
                if getattr(e, "ssd_total_gb", 0.0) > 0.0:
                    used_ssd = min(used_ssd, e.ssd_total_gb)
                e.ssd_used_gb = used_ssd
            elif getattr(e, "ssd_used_gb", None) is None:
                e.ssd_used_gb = 0.0
            
            e.lastseen = time.time()

            # Node usage report: write one line per poll
            try:
                if getattr(status, 'node_reports_file', None):
                    if e.partition  == 'compute':
                        ts_iso = datetime.datetime.now().isoformat()
                        # Queue stats for this partition
                        pending_jobs = 0
                        pending_cores = 0.0
                        pending_mem_mb = 0.0
                        try:
                            for j in list(jobs.jobs_by_id.values()):
                                if j.partition == e.partition and j.state in queue_states:
                                    pending_jobs += 1
                                    try:
                                        pending_cores += float(j.ncpu)
                                    except Exception:
                                        pending_cores += 0.0
                                    try:
                                        pending_mem_mb += float(j.mem)
                                    except Exception:
                                        pending_mem_mb += 0.0
                        except Exception:
                            # ensure we still log a line even if queue scan fails
                            pass

                        row = [
                            ts_iso,
                            e.engine_id,
                            (e.cluster_id if e.cluster_id is not False else "NA"),
                            e.partition,
                            e.status,
                            1 if getattr(e, 'managed', False) else 0,
                            1 if getattr(e, 'stopping', False) else 0,
                            int(e.cores),
                            float(e.totmem),
                            float(e.cpu_usage),
                            float(e.mem_usage),
                            float(e.load),
                            float(getattr(e, 'sys_cpu_busy', 0.0)),
                            float(getattr(e, 'sys_iowait', 0.0)),
                            float(getattr(e, 'res_cpu_reserved', 0.0)),
                            float(getattr(e, 'res_mem_reserved_mb', 0.0)),
                            int(len(getattr(e, 'jobs', []))),
                            1 if bool(getattr(e, 'has_ssd', False)) else 0,
                            float(getattr(e, 'ssd_total_gb', 0.0)),
                            float(getattr(e, 'ssd_used_gb', 0.0)),
                            float(getattr(e, 'res_ssd_reserved_gb', 0.0)),
                            float(getattr(e, 'timeleft', 0.0)),
                            float(time.time() - float(getattr(e, 'starttime', time.time()))),
                            int(pending_jobs),
                            float(pending_cores),
                            float(pending_mem_mb),
                        ]
                        try:
                            status.reports_lock.acquire()
                            status.node_reports_file.write("\t".join([str(x) for x in row]) + "\n")
                            status.node_reports_file.flush()
                        finally:
                            status.reports_lock.release()
            except Exception as ex:
                try:
                    import traceback
                    add_log_line(
                        gb,
                        f"Exception writing node usage report for engine {myid}: {ex}",
                        RED,
                    )
                    add_log_line(gb, traceback.format_exc(), RED)
                    traceback.print_exc()
                except Exception:
                    pass

            # If this engine is stopping, do not reconcile or assign; force STOP
            if e.stopping:
                self.lock.acquire()
                commands = e.pending_commands
                e.pending_commands = []
                self.lock.release()
                commands.append((zslurm_shared.STOP, None))
                return commands

            commands = e.pending_commands
            # kill if idle for 3 minutes, but not if local engine or archive engine  (except if no extra jobs)
            if idle > 0.0 and e.status == PHASING_OUT:
                add_log_line(gb, "Stopping phased out engine: " + myid, YELLOW)
                commands.append((zslurm_shared.STOP, None))
            

            if commands:
                self.lock.acquire()
                commands = e.pending_commands
                e.pending_commands = []
                self.lock.release()

            for jobid, value in current_cpu_usage.items():
                job = jobs.jobs_by_id.get(jobid, None)
                if not job is None:
                    job.current_cpu_usage = value
                    mem = current_mem_usage.get(jobid, 0.0)
                    job.current_mem_usage = mem

            # Reconcile jobs after reregister events: if engine reports a job running
            # that the manager currently does not consider RUNNING, mark it RUNNING
            # and re-reserve resources if needed.
            jobs.lock.acquire()
            try:
                for jobid in list(current_cpu_usage.keys()):
                    job = jobs.jobs_by_id.get(jobid, None)
                    if job is None:
                        continue
                    if job.state != "RUNNING":
                        # If the job was queued (PENDING/REQUEUED), re-reserve storage resources
                        if job.state in queue_states:
                            jobs.active_inuse += job.active_start_use_add
                            jobs.dcache_inuse += job.dcache_start_use_add
                            jobs.archive_inuse += job.archive_start_use_add
                        # Transition to RUNNING on this engine (increments running_jobs and updates node_id)
                        jobs.job_start(myid, job.jobid)
                        add_log_line(
                            gb,
                            f"Reconciled job {job.job_name} (id: {job.jobid}) back to RUNNING on node {myid}",
                            BLUE,
                        )
            finally:
                jobs.lock.release()

        except KeyError:
            commands = []
            if mode == zslurm_shared.STOPPING:
                add_log_line(
                    gb,
                    "Poll for unknown engine id: " + myid + ". Engine is stopping.",
                    RED,
                )
                commands.append((zslurm_shared.STOP, None))
            else:                
                add_log_line(
                    gb,
                    "Poll for unknown engine id: " + myid + ". Taking ownership.",
                    RED,
                )
                commands.append((zslurm_shared.REREGISTER, None))

        return commands

    def unregister(self, myid):
        if myid in self.engine_by_id:
            e = self.engine_by_id[myid]
            self._unregister(e, "Engine unregistered")
            add_log_line(gb, "Engine with id " + myid + " unregistered", CYAN)
        else:
            add_log_line(gb, "Unregister request for unknown engine: " + myid, RED)

        return True

    def terminate_engine(self, myid):
        self.lock.acquire()
        try:
            e = self.engine_by_id[myid]
            # mark as stopping to prevent any further scheduling/starts
            e.stopping = True
            if (
                not (e.cluster_id is None or e.cluster_id is False)
                and e.cluster_id in self.engine_by_clusterid
            ):
                self.cancel_cid(e.cluster_id)
            if e.cluster_id is False:
                self.stop_local(gb)
        finally:
            self.lock.release()

    def _unregister(self, engine, reason="Engine disappeared"):
        jobs.engine_removed(engine.engine_id, reason=reason)
        self.lock.acquire()
        try:
            self.engine_by_id.pop(engine.engine_id, "")
            self.engine_by_clusterid.pop(engine.cluster_id, "")

        finally:
            self.lock.release()


engines = EngineManager()
unregister = lambda *args: engines.unregister(*args)
poll = lambda *args: engines.poll(*args)
register = lambda *args: engines.register(*args)
list_nodes = lambda *args: engines.list_nodes(*args)
failed_node = lambda *args: engines.failed_node(*args)


def compute_autogrow_plan(all_engines, partition):    
    eligible = []
    for j in list(jobs.jobs_by_id.values()):
        if (
            j.partition == partition
            and j.state in queue_states
            and ((jobs.active_total >= (jobs.active_inuse + j.active_start_use_add)) or j.active_start_use_add == 0)
            and ((jobs.dcache_total >= (jobs.dcache_inuse + j.dcache_start_use_add)) or j.dcache_start_use_add == 0)
            and ((jobs.archive_total >= (jobs.archive_inuse + j.archive_start_use_add)) or j.archive_start_use_add == 0)
        ):
            eligible.append(j)

    unique_part = list(set([part_name for part_name,_ in status.autogrow_prefer_partitions]))

    #Reserved resources
    res_cpu = sum(float(getattr(e, 'res_cpu_reserved', 0.0)) for e in all_engines if e.partition == partition)
    res_mem_mb = sum(float(getattr(e, 'res_mem_reserved_mb', 0.0)) for e in all_engines if e.partition == partition)

    #Pending resources
    pend_cpu = sum(float(j.ncpu) for j in eligible)
    pend_mem_mb = sum(float(j.mem) for j in eligible)

    #Total capacity of the partition
    tot_cpu_cap = sum(float(getattr(e, 'cores', 0.0)) for e in all_engines if e.partition == partition)
    tot_mem_cap_mb = sum(float(getattr(e, 'totmem', 0.0)) for e in all_engines if e.partition == partition)


    #Add queued resources
    queue_cpu_cap = 0.0
    queue_mem_cap_mb = 0.0

    for part_name in unique_part:
        nqueued_engines = engines.cluster_queued_by_partition.get(part_name, 0)      
        prof = status.node_profiles.get(part_name)
        
        queue_cpu_cap += nqueued_engines * float(prof.get('cores', 0.0))
        queue_mem_cap_mb += nqueued_engines * float(prof.get('mem_gb', 0.0)) * 1024.0

        
    genoa_prof = status.node_profiles.get('genoa')
    largest_cores = int(genoa_prof.get('cores', 0))
    
    # Free cluster capacity across currently running engines after pending jobs
    free_cpu_total = max(0.0, tot_cpu_cap - res_cpu)
    free_mem_total_mb = max(0.0, tot_mem_cap_mb - res_mem_mb)
    
    #Required resources.
    #The 1.1 is to not overgrow the cluster too much and have to shrink it too soon.
    required_cpu = max(0.0, pend_cpu - free_cpu_total * 1.1 - queue_cpu_cap)
    required_mem_mb = max(0.0, pend_mem_mb - free_mem_total_mb * 1.1 - queue_mem_cap_mb)

    # Memory-per-core pressure for info only
    avail_mpc = ((tot_mem_cap_mb + queue_mem_cap_mb) / 1024.0) / max(1.0, (tot_cpu_cap + queue_cpu_cap))
    required_mpc = ((res_mem_mb + pend_mem_mb) / 1024.0) / max(1.0, (res_cpu + pend_cpu))

    
    if required_cpu <= largest_cores and tot_cpu_cap > 0: #if there is already an engine, only grow if we can fill a full engine
        return {
            'has_eligible': len(eligible) > 0,
            'eligible_n': len(eligible),
            'plan_part': None,
            'plan_nodes': 0.0,
            'plan_use_scratch': True,
            'needed_cpu': required_cpu / (genoa_prof.get('cores', 0.0)),
            'needed_mem': required_mem_mb / (genoa_prof.get('mem_gb', 0.0) * 1024.0),
            'avail_mpc': avail_mpc,
            'required_mpc': required_mpc,
            'raw_best_nodes': 0.0,
            'fat_cap_applied': False,
        }

    #parts_states_by_scratch = zslurm_shared.slurm_partition_state_counts_by_scratch(60)

    best_part = None
    best_use_scratch = True
    best_score = -1e18

    def safe_float(x, default=0.0):
        try:
            return float(x)
        except Exception:
            return default

    for part, scratch in status.autogrow_prefer_partitions:
        prof = status.node_profiles.get(part) or {}
        if not prof:
            continue

        # Scratch-aware idle nodes
        #pinfo = parts_states_by_scratch.get(part, {}) or {}
        #grp = pinfo.get('scratch' if scratch else 'no_scratch', {}) or {}
        #idle = int((grp.get('states', {}) or {}).get('IDLE', 0))
        #FIXME
        idle = 1

        # Estimate nodes needed for this profile
        ncores = safe_float(prof.get('cores', 0.0), 0.0)
        nmem_mb = safe_float(prof.get('mem_gb', 0.0), 0.0) * 1024.0
        nc = max(0.0, required_cpu / max(ncores, 1.0))
        nm = max(0.0, required_mem_mb / max(nmem_mb, 1.0))
        need_nodes = int(math.ceil(max(nc, nm))) if (required_cpu > 0.0 or required_mem_mb > 0.0) else 0

        # Scoring
        sc = 0.0

        # penalize fat by default
        if str(part).startswith('fat'):
            sc -= 4.0

        # prefer genoa slightly, detract non-genoa
        if 'genoa' not in str(part):
            sc -= 4.0

        # prefer scratch, detract non-scratch
        if not scratch:
            sc -= 1.0

        # reward partitions that have idle close to the need
        sc += float(min(min(idle, max(need_nodes, 1)), 3)) * 5.0

        # add score if adding this profile improves memory/core availability when under memory pressure
        candidate_avail_mpc = ((tot_mem_cap_mb + nmem_mb) / 1024.0) / max(1.0, (tot_cpu_cap + ncores))
        if avail_mpc < required_mpc and candidate_avail_mpc > avail_mpc:
            sc += 5.0

        if sc > best_score:
            best_score = sc
            best_part = part
            best_use_scratch = bool(scratch)

    if best_part is None:
        best_part = 'genoa'
        best_use_scratch = False

    prof = status.node_profiles.get(best_part) or {}
    try:
        nodes_cpu = max(0.0, required_cpu / float(max(1.0, float(prof.get('cores', 0.0)))))
    except Exception:
        nodes_cpu = 0.0
    try:
        nodes_mem = max(0.0, required_mem_mb / (float(max(1.0, float(prof.get('mem_gb', 0.0)))) * 1024.0))
    except Exception:
        nodes_mem = 0.0

    raw_best_nodes = int(math.ceil(max(nodes_cpu, nodes_mem))) if (required_cpu > 0.0 or required_mem_mb > 0.0) else 0
    best_nodes = raw_best_nodes

    if best_nodes > 1 and not str(best_part).startswith('fat'):
        best_nodes = max(int(math.ceil(0.2 * best_nodes)), 1)

    fat_cap_applied = False
    if str(best_part).startswith('fat') and best_nodes > 1:
        best_nodes = 1
        fat_cap_applied = True

    plan = {
        'has_eligible': len(eligible) > 0,
        'eligible_n': len(eligible),

        'plan_part': best_part,
        'plan_use_scratch': best_use_scratch,
        'plan_nodes': best_nodes,
        'best_part': best_part,
        'best_nodes': best_nodes,

        'needed_cpu': nodes_cpu,
        'needed_mem': nodes_mem,
        
        'avail_mpc': avail_mpc,
        'required_mpc': required_mpc,
        'raw_best_nodes': raw_best_nodes,
        'fat_cap_applied': fat_cap_applied,
    }
    
    return plan

def thread_check_commands():
    
    autogrow_last_plan_ts = 0.0
    autoconsolidate_last_plan_ts = 0.0

    

    while 1:
        engines.lock.acquire()
        try:
            engines._check_cluster_engines()
            all_engines = list(engines.engine_by_id.values())
        finally:
            engines.lock.release()

        now = time.time()
        for engine in all_engines:
            if now - engine.lastseen > TIMEOUT:
                # Attempt to kill SLURM allocation for this engine first without unregistering yet
                cid = engine.cluster_id
                # mark as stopping to avoid further assignments/starts and reconciliation
                engine.stopping = True
                try:
                    if cid is False:
                        # Local engine
                        engines.stop_local(gb)
                    elif cid is not None:
                        engines.cancel_cid(cid, unregister_after=False)
                except Exception as ex:
                    import traceback
                    add_log_line(gb, f"Exception when terminating engine {engine.engine_id} (cid={cid}): {ex}", RED)
                    add_log_line(gb, traceback.format_exc(), RED)
                # Wait briefly for the SLURM job to actually disappear
                try:
                    engines._wait_for_cid_termination(cid, timeout_seconds=60, poll_interval=2)
                except Exception as ex:
                    import traceback
                    add_log_line(gb, f"Exception while waiting for SLURM job {cid} to disappear: {ex}", YELLOW)
                    add_log_line(gb, traceback.format_exc(), YELLOW)
                # Now unregister and requeue jobs
                engines._unregister(engine, "Engine timed out")
                add_log_line(
                    gb, "Engine with id " + engine.engine_id + " timed out", RED
                )

        used_cpus = []
        used_mem = []
        used_load = []
        total_cpus = []
        total_mem = []
        sys_busy = []
        sys_iow = []
        ssd_used = []
        ssd_total = []
        for engine in all_engines:
            if not engine.partition == "archive":
                total_cpus.append(engine.cores)
                total_mem.append(engine.totmem / 1024.0)

                used_cpus.append(engine.cores * (engine.cpu_usage / 100.0))
                used_mem.append((engine.totmem / 1024.0) * (engine.mem_usage / 100.0))
                used_load.append(engine.cores * engine.load)
                sys_busy.append(engine.sys_cpu_busy)
                sys_iow.append(engine.sys_iowait)
                if getattr(engine, 'has_ssd', False) and getattr(engine, 'ssd_total_gb', 0.0) > 0.0:
                    ssd_total.append(float(getattr(engine, 'ssd_total_gb', 0.0)))
                    ssd_used.append(float(getattr(engine, 'ssd_used_gb', 0.0)))
        status.total_ssd = numpy.array(ssd_total, dtype=float)
        status.used_ssd = numpy.array(ssd_used, dtype=float)
        status.ssd_total_gb = float(status.total_ssd.sum()) if status.total_ssd.size else 0.0
        status.ssd_used_gb = float(status.used_ssd.sum()) if status.used_ssd.size else 0.0

        status.used_cpus = numpy.array(used_cpus)
        status.used_load = numpy.array(used_load)
        status.total_cpus = numpy.array(total_cpus, dtype=float)
        status.used_mem = numpy.array(used_mem, dtype=float)
        status.total_mem = numpy.array(total_mem, dtype=float)
        status.sys_cpu_busy = numpy.array(sys_busy, dtype=float)
        status.sys_iowait = numpy.array(sys_iow, dtype=float)

        now = time.time()
        try:            
            safety_fraction = float(status.config.get("consolidation_safety_fraction", status.consolidation_safety_fraction))
            min_engines = int(status.config.get("consolidation_min_engines", status.consolidation_min_engines))
            hysteresis_sec = float(status.config.get("consolidation_hysteresis_sec", status.consolidation_hysteresis_sec))
            cooldown_sec = float(status.config.get("consolidation_cooldown_sec", status.consolidation_cooldown_sec))
            min_runtime_sec = float(status.config.get("consolidation_min_runtime_sec", status.consolidation_min_runtime_sec))
            allow_with_queue = bool(status.config.get("consolidation_allow_with_queue", status.consolidation_allow_with_queue))
            pending_min_age_sec = float(status.config.get("consolidation_pending_min_age_sec", status.consolidation_pending_min_age_sec))
            dbg_consolidation = bool(status.config.get("consolidation_debug", status.consolidation_debug))
            keep_weights = dict(status.config.get("consolidation_keep_weights", status.consolidation_keep_weights))

            if status.autoconsolidate_enable and now - autoconsolidate_last_plan_ts > 60:
                autoconsolidate_last_plan_ts = now

                parts = set(e.partition for e in all_engines)
                for part in parts:
                    part_engines = [e for e in all_engines if e.partition == part]
                    if not part_engines:
                        continue

                    # Determine max capacity of any engine in this partition
                    try:
                        cap_cpu_max = max(int(getattr(e, 'cores', 0)) for e in part_engines)
                        cap_mem_mb_max = max(float(getattr(e, 'totmem', 0.0)) for e in part_engines)
                    except Exception:
                        cap_cpu_max = 0
                        cap_mem_mb_max = 0.0

                    pending_eligible = []
                    for j in list(jobs.jobs_by_id.values()):
                        if (
                            j.partition == part
                            and j.state in queue_states
                            and float(j.ncpu) <= float(cap_cpu_max)
                            and float(j.mem) <= float(cap_mem_mb_max)
                            and (
                                part == 'archive'
                                or not hasattr(j, 'submit_ts')
                                or (now - float(getattr(j, 'submit_ts', now)) >= pending_min_age_sec)
                            )
                            and ((jobs.active_total >= (jobs.active_inuse + j.active_start_use_add)) or j.active_start_use_add == 0)
                            and ((jobs.dcache_total >= (jobs.dcache_inuse + j.dcache_start_use_add)) or j.dcache_start_use_add == 0)
                            and ((jobs.archive_total >= (jobs.archive_inuse + j.archive_start_use_add)) or j.archive_start_use_add == 0)
                        ):
                            pending_eligible.append(j)
                      
                    if pending_eligible and (not allow_with_queue or part == 'archive'):
                        # Skip consolidation when queued jobs exist; do NOT reset cooldown.
                        # Clear hysteresis desire and unphase any prior targets so engines are usable.
                        status.consolidation_want[part] = None
                        unphased = []
                        for e in part_engines:
                            if e.status == PHASING_OUT:
                                e.status = ""
                                unphased.append(e.engine_id)
                        if dbg_consolidation:
                            try:
                                add_log_line(gb, f"[cons] part={part} skip consolidation due to {len(pending_eligible)} eligible queued job(s); unphased={len(unphased)}", CYAN)
                            except Exception:
                                pass
                        continue

                    running_jobs = [
                        j for j in list(jobs.jobs_by_id.values())
                        if j.partition == part and j.state == "RUNNING"
                    ]

                    total_req_cpu = sum(float(j.ncpu) for j in running_jobs)
                    total_req_mem_mb = sum(float(j.mem) for j in running_jobs)
                    needed_cpu = max(0, int(math.ceil(total_req_cpu * safety_fraction)))
                    needed_mem_mb = max(0, int(math.ceil(total_req_mem_mb * safety_fraction)))

                    def keep_score(e):
                        res_cpu_frac = float(getattr(e, 'res_cpu_reserved', 0.0)) / max(1.0, float(e.cores))
                        res_mem_frac = (float(getattr(e, 'res_mem_reserved_mb', 0.0)) / 1024.0) / max(1.0, float(e.totmem) / 1024.0)
                        jobs_factor = float(len(e.jobs))
                        tl = float(min(float(e.timeleft), 5 * 24 * 3600)) if hasattr(e, 'timeleft') else (5 * 24 * 3600.0)
                        time_factor = tl / float(5 * 24 * 3600)
                        w_jobs = float(keep_weights.get('jobs', 0.5))
                        w_res = float(keep_weights.get('reserves', 0.3))
                        w_time = float(keep_weights.get('time', 0.2))
                        alpha = float(keep_weights.get('reserves_alpha_cpu', 0.5))
                        return w_jobs * jobs_factor + w_res * (alpha * res_cpu_frac + (1.0 - alpha) * res_mem_frac) + w_time * time_factor

                    engines_sorted = sorted(part_engines, key=keep_score, reverse=True)

                    keep = []
                    acc_cpu = 0
                    acc_mem_mb = 0
                    for e in engines_sorted:
                        if acc_cpu >= needed_cpu and acc_mem_mb >= needed_mem_mb:
                            break
                        keep.append(e)
                        acc_cpu += int(e.cores)
                        acc_mem_mb += int(e.totmem)

                    if len(keep) < min_engines and part_engines:
                        need_more = min_engines - len(keep)
                        for e in engines_sorted[len(keep):len(keep)+need_more]:
                            keep.append(e)

                    keep_ids = set(e.engine_id for e in keep)
                    if dbg_consolidation:
                        try:
                            sum_keep_cpu = sum(int(e.cores) for e in keep)
                            sum_keep_mem_mb = sum(int(e.totmem) for e in keep)
                            add_log_line(gb, f"[cons] part={part} needed_cpu={needed_cpu} needed_mem_mb={int(needed_mem_mb)} keep={len(keep)}/{len(part_engines)} keep_cpu={sum_keep_cpu} keep_mem_mb={int(sum_keep_mem_mb)}", CYAN)
                        except Exception:
                            pass

                    unphase_changed = False
                    for e in part_engines:
                        if e.engine_id in keep_ids and e.status == PHASING_OUT:
                            e.status = ""
                            unphase_changed = True

                    needs_phaseout = len(keep) < len(part_engines)
                    want_ts = status.consolidation_want.get(part)
                    last_action_ts_by_part = getattr(status, 'consolidation_last_action_ts_by_part', {}) or {}
                    last_action_ts = float(last_action_ts_by_part.get(part, 0.0))
                    phase_changed = False
                    if needs_phaseout and (now - last_action_ts) >= cooldown_sec:
                        if want_ts is None:
                            status.consolidation_want[part] = now
                            if dbg_consolidation:
                                try:
                                    add_log_line(gb, f"[cons] part={part} start hysteresis window ({int(hysteresis_sec)}s)", CYAN)
                                except Exception:
                                    pass
                        elif (now - want_ts) >= hysteresis_sec:
                            for e in part_engines:
                                if e.engine_id not in keep_ids:
                                    if (now - float(e.starttime)) >= min_runtime_sec:
                                        if e.status != PHASING_OUT:
                                            e.status = PHASING_OUT
                                            phase_changed = True                            
                                
                            if unphase_changed or phase_changed:
                                # update per-part cooldown timestamp
                                last_action_ts_by_part[part] = now
                                status.consolidation_last_action_ts_by_part = dict(last_action_ts_by_part)
                    else:
                        if not needs_phaseout:
                            status.consolidation_want[part] = None                            

        except Exception:
            import traceback
            traceback.print_exc()
        # Autogrow controller
        try:
            now = time.time()            
            if autogrow_last_plan_ts < now - 60.0 and status.autogrow_enable:
                autogrow_last_plan_ts = now
                
                plan = compute_autogrow_plan(all_engines, "compute")
                cooldown_remaining_compute = max(0.0, float(status.autogrow_cooldown_sec) - (now - float(status.autogrow_last_action_compute_ts)))
                
                if plan.get('best_nodes', 0) > 0:
                    plan_compute = f"grow: {plan.get('best_nodes', 0)} @ {plan.get('best_part', 'compute')}, cooldown {int(cooldown_remaining_compute)}s left"                                            
                else:
                    plan_compute = f"no nodes needed"

                if cooldown_remaining_compute <= 0: # Autogrow cooldown expired, lets see if we need to start some engines
                    if plan.get('best_part',None) is not None:
                        to_start = plan.get('best_nodes', 0)
                        if to_start > 0:
                            try:
                                max_compute_nodes = int(status.config.get('autogrow_max_compute_nodes', 40))
                            except Exception:
                                max_compute_nodes = 40
                            current_compute = int(getattr(engines, 'cluster_running_engines', 0)) + int(getattr(engines, 'cluster_queued_engines', 0))
                            cap_remaining = max(0, int(max_compute_nodes) - int(current_compute))
                            clamped = False
                            if to_start > cap_remaining:
                                to_start = cap_remaining
                                clamped = True
                            if to_start > 0:
                                msg = f"Autogrow: starting {to_start} node(s) on {plan.get('best_part')}"
                                if clamped:
                                    msg += f" (cap {current_compute}+{to_start} <= {max_compute_nodes})"
                                add_log_line(
                                    gb,
                                    msg,
                                    CYAN,
                                )
                                constraint = "scratch-node" if bool(plan.get('plan_use_scratch', True)) else None
                                engines.start_slurm(gb, to_start, status.autogrow_runtime_default, plan.get('best_part'), constraint, 0)
                                status.autogrow_last_action_compute_ts = now
                                plan_compute = f"growed: {to_start} @ {plan.get('best_part', 'compute')}"

                
                staging_pending = 0
                try:
                    for job in list(jobs.jobs_by_id.values()):
                        part = str(getattr(job, 'partition', '') or '')
                        if part in ('archive', 'staging') and getattr(job, 'state', '') in queue_states:
                            staging_pending += 1
                except Exception:
                    staging_pending = 0

                desired_staging = 0
                if staging_pending >= 50:
                    desired_staging = 4
                elif staging_pending > 0:
                    desired_staging = 1
                
                queued_map = getattr(engines, 'cluster_queued_by_partition', {}) or {}
                running_map = getattr(engines, 'cluster_running_by_partition', {}) or {}
                current_staging = int(queued_map.get('staging', 0)) + int(running_map.get('staging', 0))

                cooldown_remaining_staging = max(0.0, float(status.autogrow_cooldown_sec) - (now - float(status.autogrow_last_action_staging_ts)))

                if desired_staging > current_staging:
                    plan_staging = f"grow {desired_staging  - current_staging}, cooldown {int(cooldown_remaining_staging)}s left"
                else:
                    plan_staging = f"no nodes needed"
                
                
                if cooldown_remaining_staging <= 0:    
                    if desired_staging > 0:
                        if desired_staging > current_staging:
                            to_start_staging = min(desired_staging - current_staging, 4 - current_staging)
                            if to_start_staging > 0:
                                add_log_line(
                                    gb,
                                    f"Staging autogrow: {staging_pending} archive jobs pending, starting 1 of {to_start_staging} nodes.",
                                    CYAN,
                                )
                                engines.start_slurm(gb, 1, status.autogrow_runtime_default, "staging", None, 1)                                
                                status.autogrow_last_action_staging_ts = now
                                plan_staging = f"growed: 1"


                status.autogrow_plan =f"Compute: {plan_compute} | Staging: {plan_staging}"              

        except Exception:
            import traceback
            traceback.print_exc()            

        time.sleep(5)


def thread_start_manager_server(port, rpc_path):
    request_handeler=SimpleXMLRPCRequestHandler
    request_handeler.rpc_paths=(f"/{rpc_path}",)
    server=SimpleXMLRPCServer(
        ("", int(port)),
        logRequests=False,
        allow_none=True,
        requestHandler=request_handeler) 
    server.register_introspection_functions()

    server.register_function(register, "register")
    server.register_function(failed_node, "failed_node")
    server.register_function(unregister, "unregister")
    server.register_function(poll, "poll")
    server.register_function(request_jobs, "request_jobs")
    server.register_function(job_finished, "job_finished")
    server.register_function(can_run_assigned_job, "can_run_assigned_job")

    Servers.manager_server = server
    add_log_line(gb, f"[manager] started on port {port} with rpc path {rpc_path}", GREEN)
    server.serve_forever()


def start_server(port, rpcpath):
    t = threading.Thread(target=thread_start_manager_server, args=(port, rpcpath))
    t.daemon = True
    t.start()

    t = threading.Thread(target=thread_start_job_server, args=(port + 1, rpcpath))
    t.daemon = True
    t.start()

    t = threading.Thread(target=thread_check_commands)
    t.daemon = True
    t.start()


# create a bordered window
def border_win(parent, height, width, x, y, **kwargs):
    if not "notop" in kwargs:
        parent.hline(x, y, ".", width)
        height -= 1
        x += 1
    if not "nobottom" in kwargs:
        parent.hline(x + height - 1, y, ".", width)
        height -= 1
    if not "noleft" in kwargs:
        parent.vline(x, y, ".", height)
        width -= 1
        y += 1
    if not "noright" in kwargs:
        parent.vline(x, y + width - 1, ".", height)
        width -= 1
    w = parent.subwin(height, width, x, y)
    w.noutrefresh()
    parent.refresh()
    return w


# threaded output of a file to the log window
def thread_log_output(file, gb, color, filter):
    while 1:
        line = file.readline()
        if not line:
            break

        if filter and len([f for f in filter if f in line]) > 0:
            pass
        else:
            add_log_line(gb, line.decode("utf-8", "ignore"), color)


def log_output(file, gb, color=0, filter=[]):
    t = _thread.start_new_thread(thread_log_output, (file, gb, color, filter))
    return t


def get_stat_str(stats):
    cpu, memp, mempmax, mem, memmax = stats
    if cpu < 0.50 or mempmax > 0.90:
        color = RED
    elif cpu < 0.80 or mempmax > 0.70:
        color = YELLOW
    else:
        color = GREEN

    descr = (
        "("
        + "{:.2%}".format(cpu)
        + " CPU, "
        + "{:.2%}".format(memp)
        + "/"
        + "{:.2%}".format(mempmax)
        + "/"
        + "{}".format(int(mem))
        + "MB MEM)"
    )
    return descr, color


# thread display status
def thread_display_status(gb, status):
    counter = 9
    completed = queue = running = 0
    while not status.stop_status_display:
        counter += 1
        local_running = engines.check_local_engine()
        equeue = engines.cluster_queued_engines
        erunning = engines.cluster_running_engines

        equeue_archive = engines.cluster_queued_engines_archive
        erunning_archive = engines.cluster_running_engines_archive

        njobs, nrunning, neligible, nfailed, ncompleted = jobs.get_job_stats()

        # active_engines = jobs.active_engines()
        # active_engines.discard(mylocal_id)
        # eactive = len(active_engines)

        if counter >= 10:
            rc = None
            try:
                sys.stdout = open(os.devnull, "w")
                rc = ipyparallel.Client(timeout=0.5)
                queue = rc.queue_status()
                unassigned = queue["unassigned"]
                del queue["unassigned"]

                completed = sum(
                    [
                        value["completed"]
                        for key, value in list(queue.items())
                        if isinstance(key, int)
                    ],
                    0,
                )
                nqueue = sum(
                    [
                        value["queue"]
                        for key, value in list(queue.items())
                        if isinstance(key, int)
                    ],
                    0,
                )
                running = sum(
                    [
                        value["tasks"]
                        for key, value in list(queue.items())
                        if isinstance(key, int)
                    ],
                    0,
                )
                queue = nqueue + unassigned
            except Exception as e:
                pass
            finally:
                sys.stdout = sys.__stdout__
                if not rc is None:
                    rc.close()
            counter = 0

        gb.lock.acquire()
        try:
            if gb.wcom_status and gb.wcom_status.getmaxyx()[1] > 50:
                sizey = gb.wcom_status.getmaxyx()[1]
                gb.wcom_status.erase()
                compute_running = nrunning.get("compute", 0)
                compute_eligible = neligible.get("compute", 0)
                compute_total = njobs.get("compute", 0)
                compute_completed = ncompleted.get("compute", 0)
                compute_failed = nfailed.get("compute", 0)

                archive_running = nrunning.get("archive", 0)
                archive_eligible = neligible.get("archive", 0)
                archive_total = njobs.get("archive", 0)
                archive_completed = ncompleted.get("archive", 0)
                archive_failed = nfailed.get("archive", 0)

                gb.wcom_status.addstr(1, 1, "Compute jobs: ")
                gb.wcom_status.addstr(
                    1,
                    18,
                    "%d/%d/%d (%d completed, %d failed)"
                    % (
                        compute_running,
                        compute_eligible,
                        compute_total,
                        compute_completed,
                        compute_failed,
                    ),
                    curses.color_pair(GREEN),
                )
                gb.wcom_status.addstr(2, 1, "Archive jobs: ")
                gb.wcom_status.addstr(
                    2,
                    18,
                    "%d/%d/%d (%d completed, %d failed)"
                    % (
                        archive_running,
                        archive_eligible,
                        archive_total,
                        archive_completed,
                        archive_failed,
                    ),
                    curses.color_pair(GREEN),
                )


                total_ssd_arr = numpy.asarray(getattr(status, 'total_ssd', []), dtype=float)
                used_ssd_arr = numpy.asarray(getattr(status, 'used_ssd', []), dtype=float)


                
                if total_ssd_arr.size:
                    gb.wcom_status.addstr(10, 1, "SSD usage (TB): ")
                    ssd_display = "n/a"
                    ssd_color = GREEN

                    ssd_used_sum = float(used_ssd_arr.sum())
                    ssd_total_sum = float(total_ssd_arr.sum())
                    ratio = used_ssd_arr / total_ssd_arr
                    

                    if total_ssd_arr.size > 5:
                        
                        ssd_usage = numpy.percentile(ratio, [0, 25, 50, 75, 100])
                        if (
                            ssd_usage[-1] > 0.99
                            or ssd_usage[-2] > 0.95
                            or ssd_usage[-3] > 0.90
                        ):
                            ssd_color = RED
                        elif (
                            ssd_usage[-1] > 0.90
                            or ssd_usage[-2] > 0.80
                            or ssd_usage[-3] > 0.70
                        ):
                            ssd_color = YELLOW

                        format_str = "(" + " - ".join(["%.2f"] * 5) + ")"
                    else:                                                
                        if ratio.size and ratio.max() > 0.95:
                            ssd_color = RED
                        elif ratio.size and ratio.max() > 0.75:
                            ssd_color = YELLOW

                        ssd_usage = numpy.sort(ratio)
                        format_str = "(" + " - ".join(["%.2f"] * len(ratio)) + ")"



                    ssd_display = (
                        "%.1f/%.1f " + format_str
                    ) % ((ssd_used_sum / 1024.0, ssd_total_sum / 1024.0) + tuple(ssd_usage.tolist()))

                    gb.wcom_status.addstr(
                        10,
                        18,
                        ssd_display[: max(0, sizey - 2)],
                        curses.color_pair(ssd_color),
                    )

                gb.wcom_status.addstr(4, 1, "Local engine: ")
                if not local_running:
                    gb.wcom_status.addstr(
                        4, 18, "NOT RUNNING", curses.color_pair(YELLOW)
                    )
                else:
                    gb.wcom_status.addstr(4, 18, "RUNNING", curses.color_pair(GREEN))

                gb.wcom_status.addstr(5, 1, "SLURM engines: ")
                enodes = erunning + equeue

                cpu_c = GREEN
                mem_c = GREEN
                load_c = GREEN

                if len(status.used_cpus) > 5:
                    cpu_usage = numpy.percentile(
                        status.used_cpus / status.total_cpus, [0, 25, 50, 75, 100]
                    )
                    mem_usage = numpy.percentile(
                        status.used_mem / status.total_mem, [0, 25, 50, 75, 100]
                    )
                    load = numpy.percentile(
                        status.used_load / status.total_cpus, [0, 25, 50, 75, 100]
                    )

                    if cpu_usage[1] < 0.5 or cpu_usage[2] < 0.75:
                        cpu_c = YELLOW
                    if cpu_usage[1] < 0.1 or cpu_usage[2] < 0.2:
                        cpu_c = RED

                    if (
                        mem_usage[-1] > 0.9
                        or mem_usage[-2] > 0.8
                        or mem_usage[-3] > 0.7
                    ):
                        mem_c = YELLOW
                    if (
                        mem_usage[-1] > 0.99
                        or mem_usage[-2] > 0.95
                        or mem_usage[-3] > 0.9
                    ):
                        mem_c = RED

                    if load[-1] > 1.1:
                        load_c = YELLOW
                    if load[2] > 1.1:
                        load_c = RED
                    format_str = "(" + " - ".join(["%.2f"] * 5) + ")"
                elif len(status.used_cpus) > 0:
                    cpu_usage = numpy.sort(status.used_cpus / status.total_cpus)
                    mem_usage = numpy.sort(status.used_mem / status.total_mem)
                    load = numpy.sort(status.used_load / status.total_cpus)
                    if cpu_usage[0] < 0.5:
                        cpu_c = YELLOW
                    elif cpu_usage[0] < 0.1:
                        cpu_c = RED

                    if mem_usage[-1] > 0.9:
                        cpu_c = YELLOW
                    elif mem_usage[-1] > 0.95:
                        cpu_c = RED

                    if load[-1] > 1.1:
                        load_c = YELLOW
                    elif load[-1] > 1.4:
                        load_c = RED

                    format_str = (
                        "(" + " - ".join(["%.2f"] * len(status.used_cpus)) + ")"
                    )
                else:
                    format_str = "%s"
                    cpu_usage = ("",)
                    mem_usage = ("",)
                    load = ("",)

                gb.wcom_status.addstr(5, 30, "Archive engines: ")
                enodes_archive = erunning_archive + equeue_archive

                gb.wcom_status.addstr(7, 1, "CPU usage: ")
                gb.wcom_status.addstr(
                    7,
                    18,
                    ("%d/%d " + format_str)
                    % (
                        (numpy.sum(status.used_cpus), numpy.sum(status.total_cpus))
                        + tuple(cpu_usage)
                    ),
                    curses.color_pair(cpu_c),
                )

                gb.wcom_status.addstr(8, 1, "Load: ")
                gb.wcom_status.addstr(
                    8,
                    18,
                    ("%.0f/%d " + format_str)
                    % (
                        (numpy.sum(status.used_load), numpy.sum(status.total_cpus))
                        + tuple(load)
                    ),
                    curses.color_pair(load_c),
                )

                gb.wcom_status.addstr(9, 1, "Mem usage (GB): ")
                gb.wcom_status.addstr(
                    9,
                    18,
                    ("%d/%d " + format_str)
                    % (
                        (numpy.sum(status.used_mem), numpy.sum(status.total_mem))
                        + tuple(mem_usage)
                    ),
                    curses.color_pair(mem_c),
                )

                if numpy.sum(status.total_cpus) > 0 and len(status.sys_cpu_busy) == len(status.total_cpus):
                    xav = float((status.sys_cpu_busy * status.total_cpus).sum() / numpy.sum(status.total_cpus))
                    yav = float((status.sys_iowait * status.total_cpus).sum() / numpy.sum(status.total_cpus))
                else:
                    xav = 0.0
                    yav = 0.0
                
                gb.wcom_status.addstr(11, 1, "Sys CPU/IO (%): ")
                gb.wcom_status.addstr(
                    11,
                    18,
                    "%.0f | %.0f" % (xav, yav),
                    curses.color_pair(GREEN),
                )

                # Mem reserved (GB)  place inside CPU/Load/Mem block (just after Mem usage)
                try:
                    rem_gb = 0.0
                    tot_gb = 0.0
                    for e in list(engines.engine_by_id.values()):
                        if e.partition != "archive":
                            rem_gb += float(getattr(e, 'res_mem_reserved_mb', 0.0)) / 1024.0
                            tot_gb += float(e.totmem) / 1024.0
                    gb.wcom_status.addstr(11, 30, "Mem resv (GB): ")
                    gb.wcom_status.addstr(
                        11,
                        48,
                        "%d/%d" % (int(rem_gb), int(tot_gb)),
                        curses.color_pair(GREEN),
                    )
                except Exception:
                    pass

                

                if not rc is None:
                    gb.wcom_status.addstr(12, 1, "IPEngines: ")
                    gb.wcom_status.addstr(
                        12,
                        18,
                        "%d / %d / %d" % (running, queue, completed),
                        curses.color_pair(GREEN),
                    )

                gb.wcom_status.addstr(13, 1, "Archive: ")
                gb.wcom_status.addstr(14, 1, "Active: ")
                gb.wcom_status.addstr(15, 1, "DCache: ")
                gb.wcom_status.addstr(
                    13,
                    18,
                    "%d / %d" % (jobs.archive_inuse, jobs.archive_total),
                    curses.color_pair(GREEN),
                )
                gb.wcom_status.addstr(
                    14,
                    18,
                    "%d / %d" % (jobs.active_inuse, jobs.active_total),
                    curses.color_pair(GREEN),
                )
                gb.wcom_status.addstr(
                    15,
                    18,
                    "%d / %d" % (jobs.dcache_inuse, jobs.dcache_total),
                    curses.color_pair(GREEN),
                )


                # Consolidation status (compute only if present)
                try:
                    gb.wcom_status.addstr(17, 1, "Consolidation: ")
                    if not status.autoconsolidate_enable:
                        color = YELLOW
                        txt = "disabled"
                        gb.wcom_status.addstr(17, 18, txt, curses.color_pair(color))                
                    else:                    
                        p_eng = [e for e in list(engines.engine_by_id.values()) if e.partition == 'compute']
                        m = len(p_eng)
                        k = len([e for e in p_eng if e.status == PHASING_OUT])
                        n = m - k
                        gb.wcom_status.addstr(
                            17,
                            18,
                            "keep %d/%d | phased-out %d" % (n, m, k),
                            curses.color_pair(GREEN),
                        )
                    # Autogrow status: display closeness to adding a node
                    
                    gb.wcom_status.addstr(18, 1, "Autogrow: ")
                    if not status.autogrow_enable:
                        color = YELLOW
                        txt = "disabled"
                    else:
                        plan = getattr(status, 'autogrow_plan', None)
                        
                        if not plan:
                            color = CYAN
                            txt = "no data"                                    
                        else:
                            color = CYAN
                            txt = plan
                    gb.wcom_status.addstr(18, 18, txt, curses.color_pair(color))
                    
                    
                      
                except Exception:
                    pass

                color = GREEN if equeue == 0 else YELLOW
                gb.wcom_status.addstr(
                    5, 18, "%d/%d" % (erunning, enodes), curses.color_pair(color)
                )
                color_archive = GREEN if equeue_archive == 0 else YELLOW
                gb.wcom_status.addstr(
                    5,
                    48,
                    "%d/%d" % (erunning_archive, enodes_archive),
                    curses.color_pair(color_archive),
                )

                gb.wcom_status.refresh()
        except Exception as e:
            add_log_line(gb, "Exception in display thread: " + str(e), RED)
        finally:
            gb.lock.release()
        time.sleep(1)


def start_status_display(gb, status):
    status.stop_status_display = False
    t = threading.Thread(target=thread_display_status, args=(gb, status))
    t.daemon = True
    t.start()
    return t


def stop_status_display(status, t):
    set_status_message(gb, "Waiting for status thread to terminate..", YELLOW)
    status.stop_status_display = True
    t.join()


# initialize windows/lock, etc.
def build_windows(gb):
    if not gb.lock:
        gb.lock = threading.RLock()
    gb.lock.acquire()
    try:
        # clear screen in curses does not work somehow.
        print("\033[2J")

        gb.scr.erase()
        gb.scr.refresh()
        curses.start_color()
        # Invisibility mode of the cursor may not be supported by the terminal
        try:
            curses.curs_set(0)
        except:
            pass
        # Enable keypad mode globally to better capture special keys
        try:
            gb.scr.keypad(True)
        except Exception:
            pass

        # curses getmaxyx does not work for resize screens to smaller sizes. Use stty instead.
        rows, columns = os.popen("stty size", "r").read().split()
        size = (int(rows), int(columns))

        com_win_rows = max(0, min(20, size[0] - 2))
        status_win_rows = max(0, min(2, size[0] - com_win_rows))
        log_win_rows = max(0, size[0] - com_win_rows - status_win_rows)

        # init colors
        curses.init_pair(RED, curses.COLOR_RED, curses.COLOR_BLACK)
        curses.init_pair(GREEN, curses.COLOR_GREEN, curses.COLOR_BLACK)
        curses.init_pair(YELLOW, curses.COLOR_YELLOW, curses.COLOR_BLACK)
        curses.init_pair(MAGENTA, curses.COLOR_MAGENTA, curses.COLOR_BLACK)
        curses.init_pair(BLUE, curses.COLOR_BLUE, curses.COLOR_BLACK)
        curses.init_pair(CYAN, curses.COLOR_CYAN, curses.COLOR_BLACK)

        # log window
        if log_win_rows > 2:
            gb.wlog = border_win(gb.scr, log_win_rows, size[1], 0, 0)
            gb.wlog_size = gb.wlog.getmaxyx()
            gb.wlog_currow = 0
            gb.wlog.setscrreg(0, gb.wlog_size[0] - 1)
            gb.wlog.idlok(True)
            gb.wlog.scrollok(True)
            gb.wlog.refresh()
        else:
            gb.wlog = None

        # com window
        if com_win_rows > 11:
            com_win_cols = size[1] / 2
            if com_win_cols > 55:
                comlog_win_cols = size[1] - 55
                com_win_cols = 55
                gb.wcom_status = border_win(
                    gb.scr,
                    com_win_rows,
                    comlog_win_cols,
                    log_win_rows,
                    com_win_cols,
                    notop=True,
                    noleft=True,
                )
                gb.wcom_status.refresh()
            else:
                com_win_cols = size[1]
                gb.wcom_status = None

            gb.wcom = border_win(
                gb.scr, com_win_rows, com_win_cols, log_win_rows, 0, notop=True
            )
            gb.wcom.refresh()
        else:
            gb.wcom = None
            gb.wcom_status = None

        # status window
        if status_win_rows == 2:
            gb.wstatus = gb.scr.subwin(
                status_win_rows, size[1], log_win_rows + com_win_rows, 0
            )
            gb.wstatus.refresh()
        else:
            gb.wstatus = None

        init_commands(gb)
        gb.scr.refresh()
    except:
        pass
    finally:
        gb.lock.release()


# add line to log windows
def add_log_line(gb, line, color=0):
    timestring = time.strftime("%H:%M:%S", time.localtime())
    try:
        if isinstance(line, bytes):
            line = str(line.decode("utf-8"))
    except UnicodeDecodeError:
        line = "Invalid line format (unicode) encountered"
        color = RED

    line = timestring + ": " + line
    gb.lock.acquire()
    try:
        if line and line[-1] == "\n":
            gb.log_file.write(line)
        else:
            gb.log_file.write(line + "\n")
        gb.log_file.flush()
        if gb.wlog:
            line = line[: (gb.wlog_size[1] - 2)]
            if line and line[-1] == "\n":
                line = line[:-1]
            if gb.wlog_currow < gb.wlog_size[0]:
                gb.wlog.addstr(gb.wlog_currow, 1, line, curses.color_pair(color))
                gb.wlog_currow += 1
            else:
                gb.wlog.scroll()
                gb.wlog.addstr(gb.wlog_size[0] - 1, 1, line, curses.color_pair(color))
            gb.wlog.refresh()
    except Exception as e:
        pass
    finally:
        gb.lock.release()


# set status message iin status window
def set_status_message(gb, line, color=0):
    gb.lock.acquire()
    try:
        if gb.wstatus:
            gb.wstatus.addstr(0, 0, line, curses.color_pair(color))
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception as e:
        pass
    finally:
        gb.lock.release()


def set_progress_bar(gb, s):
    gb.lock.acquire()
    try:
        if gb.wstatus:
            if s < 0.0 or s > 1.0:
                add_log_line(gb, "Progress bar value out of range: " + str(s), RED)
            else:
                size = gb.wstatus.getmaxyx()
                total_i = size[1] - 3
                full_i = int(s * total_i)
                rem_i = total_i - full_i
                line = "[" + ("#" * full_i) + (" " * rem_i) + "]"
                gb.wstatus.clrtoeol()
                gb.wstatus.refresh()
    except Exception as e:
        add_log_line(gb, "Exception in set_progress_bar: " + str(e), RED)
    finally:
        gb.lock.release()


def stop_progress_bar(gb):
    gb.lock.acquire()
    try:
        if gb.wstatus:
            gb.wstatus.addstr(1, 0, "")
            gb.wstatus.clrtoeol()
            gb.wstatus.refresh()
    except Exception as e:
        pass
    finally:
        gb.lock.release()


# print commands in command windows
def init_commands(gb):
    gb.lock.acquire()
    try:
        if not gb.wcom is None:
            try:
                max_rows, _ = gb.wcom.getmaxyx()
            except Exception:
                max_rows = 20
            inst_row = max(1, min(max_rows - 2, 18))
            gb.wcom.addstr(1, 1, "Start/stop local engine: s/x")
            gb.wcom.addstr(2, 1, "Start/stop SLURM engines: d/c [n]")
            gb.wcom.addstr(3, 1, "Toggle auto consolidate: a")
            gb.wcom.addstr(4, 1, "Toggle phaseout engine: o")
            
            gb.wcom.addstr(6, 1, "Prioritize jobs: p")
            gb.wcom.addstr(7, 1, "Deprioritize jobs: n")
            gb.wcom.addstr(8, 1, "Set archive/active/dcache total capacity: 1/2/3")
            gb.wcom.addstr(9, 1, "Toggle last in/first out: l")
            gb.wcom.addstr(10, 1, "Set memory optimization job search window: m")
            gb.wcom.addstr(11, 1, "Toggle autogrow: g")
            gb.wcom.addstr(12, 1, "Shutdown: q")
            gb.wcom.addstr(inst_row, 1, "Instance:")
            gb.wcom.refresh()
    except Exception as e:
        pass
    finally:
        gb.lock.release()


def render_control_status(gb, status):
    gb.lock.acquire()
    try:
        if gb.wcom:
            try:
                rows, width = gb.wcom.getmaxyx()
            except Exception:
                rows, width = (20, 80)
            right = max(2, width - 2)
            def draw_right(row, text, color):
                try:
                    t = str(text)
                    field_width = max(4, min(24, right - 1))
                    field_start = max(1, right - field_width)
                    gb.wcom.move(row, field_start)
                    gb.wcom.clrtoeol()
                    start = max(field_start, right - len(t))
                    gb.wcom.addstr(row, start, t, curses.color_pair(color))
                except Exception:
                    pass
            col = GREEN if getattr(status, 'autoconsolidate_enable', True) else YELLOW
            draw_right(3, ("ON" if status.autoconsolidate_enable else "OFF"), col)
            col = GREEN if status.lastin_first else YELLOW
            draw_right(9, ("ON" if status.lastin_first else "OFF"), col)
            try:
                val = int(status.prio_fillmem_context)
            except Exception:
                val = 0
            draw_right(10, str(val), GREEN)
            col = GREEN if getattr(status, 'autogrow_enable', True) else YELLOW
            draw_right(11, ("ON" if status.autogrow_enable else "OFF"), col)
            try:
                inst_row = max(1, min(rows - 2, 18))
                inst_name = str(getattr(status, 'instance_name', '') or '-')
                gb.wcom.move(inst_row, 1)
                gb.wcom.clrtoeol()
                gb.wcom.addstr(inst_row, 1, f"Instance: {inst_name}", curses.color_pair(GREEN))
            except Exception:
                pass
            gb.wcom.refresh()
    except Exception:
        pass
    finally:
        gb.lock.release()


# enter command in command window
def enter_command(gb, argument="Command: "):
    if gb.wcom is None:
        return
    gb.lock.acquire()
    try:
        command = ""
        size = gb.wcom.getmaxyx()
        x = size[0] - 1
        # Compute available width and draw a truncated prompt if needed
        width = max(0, size[1] - 2)
        shown_arg = argument[:width]
        gb.wcom.addstr(x, 1, shown_arg)
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
        # Ensure keypad mode so special keys like KEY_ENTER/KEY_BACKSPACE are recognized
        try:
            gb.scr.keypad(True)
        except Exception:
            pass
        while 1:
            gb.lock.release()
            c = gb.scr.getch()
            gb.lock.acquire()
            # Enter pressed (handle LF, CR, keypad enter)
            if c in (curses.ascii.LF, curses.ascii.CR, getattr(curses, 'KEY_ENTER', -1)):
                break
            # Backspace/Delete handling (cover multiple keycodes)
            elif c in (curses.ascii.DEL, curses.ascii.BS, 127, getattr(curses, 'KEY_BACKSPACE', -1)):
                command = command[:-1]
            else:
                if c > 256:
                    # Ignore unsupported special keys
                    set_status_message(gb, "Keycode not supported: " + str(c))
                else:
                    command += chr(c)
            # Redraw prompt + current command, truncated to fit available width
            avail_cmd = max(0, width - len(shown_arg))
            shown_cmd = command[-avail_cmd:] if avail_cmd > 0 else ""
            gb.wcom.addstr(x, 1, shown_arg + shown_cmd)
            gb.wcom.clrtoeol()
            gb.wcom.refresh()

        gb.wcom.addstr(x, 1, "")
        gb.wcom.clrtoeol()
        gb.wcom.refresh()
    except:
        pass
    finally:
        gb.lock.release()
    return command


def get_number(gb, argument, default=0):
    cmd = enter_command(gb, argument)
    if not cmd:
        n = default
    else:
        try:
            n = int(cmd)
        except ValueError:
            set_status_message(gb, "Number of engines should be a number", YELLOW)
            n = None
    return n


def _is_port_available(host, port):
    try:
        base_port = int(port)
    except Exception:
        base_port = int(zslurm_shared.port)

    for candidate in (base_port, base_port + 1):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.settimeout(1.0)
                if s.connect_ex((host, candidate)) == 0:
                    return False
        except Exception:
            return True
    return True


def _find_available_port(bind_host, start_port, attempts=100):
    try:
        base_port = int(start_port)
    except Exception:
        base_port = int(zslurm_shared.port)

    for offset in range(attempts):
        candidate = base_port + (offset * 2)
        if _is_port_available(bind_host, candidate):
            return candidate
    raise RuntimeError(f"Unable to find available port near {start_port} on host {bind_host}")


def _ping_instance(instance_name, timeout=10.0):
    try:
        url = zslurm_shared.get_manager_url(instance=instance_name)
        proxy = zslurm_shared.TimeoutServerProxy(url, timeout=timeout, allow_none=True)
        proxy.system.listMethods()
        return True
    except Exception:
        return False


def _cleanup_unreachable_instances():
    names = zslurm_shared.get_instance_names()
    now = time.time()
    for name in list(names):
        if _ping_instance(name):
            continue
        try:
            zslurm_shared.remove_instance(name)
        except Exception as ex:
            import traceback
            traceback.print_exc()


def _ensure_unique_instance(address):
    #ensure instance name does not exist yet
    instance_name = f'zslurm_{address}'
    pos = 0
    active = zslurm_shared.get_instance_names()
    while instance_name in active:
        pos += 1
        instance_name = f"zslurm_{address}_{pos}"
        
    base_port = zslurm_shared.port

    free_port = _find_available_port(address, base_port)
    inst_cfg = zslurm_shared.set_instance_config(instance_name, {
        "bind_host": address,
        "advertise_host": address,
        "base_port": free_port,
    })

    return instance_name


def main(scr, *args, **kwds):
    _cleanup_unreachable_instances()
    address = zslurm_shared.get_hostname()    
    instance_name =  _ensure_unique_instance(address)
    config = zslurm_shared.get_config(instance=instance_name)



    status.instance_name = instance_name
    status.config = config
    if 'autogrow_max_compute_nodes' not in status.config:
        status.config['autogrow_max_compute_nodes'] = 40

    def _remove_own_instance():
        try:
            if status.instance_name:
                zslurm_shared.remove_instance(status.instance_name)
        except Exception as ex:
            try:
                sys.stderr.write(f"Failed to remove instance file for '{instance_name}' on exit: {ex}\n")
                sys.stderr.write(traceback.format_exc())
                sys.stderr.flush()
            except Exception:
                pass

    try:
        atexit.register(_remove_own_instance)
    except Exception as ex:
        try:
            sys.stderr.write(f"Failed to register exit cleanup: {ex}\n")
            sys.stderr.write(traceback.format_exc())
            sys.stderr.flush()
        except Exception:
            pass

    status.set_address(address, int(config["port"]))
    current_date = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')
    status.reports_file = open(config.get("reports_file_prefix", "report") + f'-{current_date}.tsv', "w")
    fieldnames = [
        "jobname",
        "comment",
        "retcode",
        "machine",
        "jobid",
        "starttime",
        "endtime",
        "cores_reserved",
        "mem_reserved_mb",
        "input_mb",
        "output_file",
    ]
    for lab in ["uss", "rss", "vms", "cpu_percentage"]:
        for j in [0, 5, 25, 50, 75, 95, 100]:
            fieldnames.append(lab + "_" + str(j))
    fieldnames.extend(
        [
            "runtime",
            "montime",
            "avg_cpu_percentage",
            "nthreads_avg",
            "nthreads_max",
            "user",
            "system",
            "maxrss",
            "mon_user",
            "mon_system",
            "iowait",
            "read_count",
            "write_count",
            "read_bytes",
            "write_bytes",
            "memory_over_time"
        ]
    )
    status.reports_file.write("\t".join(fieldnames) + "\n")
    status.reports_file.flush()

    # Open node usage report file and write header
    try:
        if bool(config.get("node_reports_enable", True)):
            status.node_reports_file = open(config.get("node_reports_file_prefix",'node_usage') + f'-{current_date}.tsv', "w")
            node_fields = [
                "ts_iso",
                "engine_id",
                "cluster_id",
                "partition",
                "status",
                "managed",
                "stopping",
                "cores",
                "totmem_mb",
                "cpu_pct",
                "mem_pct",
                "load",
                "sys_cpu_busy_pct",
                "sys_iowait_pct",
                "res_cores_reserved",
                "res_mem_reserved_mb",
                "jobs_running",
                "has_ssd",
                "ssd_total_gb",
                "ssd_used_gb",
                "res_ssd_reserved_gb",                
                "timeleft_sec",
                "uptime_sec",
                "pending_jobs",
                "pending_cores",
                "pending_mem_mb",
            ]
            status.node_reports_file.write("\t".join(node_fields) + "\n")
            status.node_reports_file.flush()
    except Exception as ex:
        try:
            import traceback
            add_log_line(gb, f"Exception opening node usage report file: {ex}", RED)
            add_log_line(gb, traceback.format_exc(), RED)
            traceback.print_exc()
        except Exception:
            pass

    gb.scr = scr
    gb.log_file = open("cluster.log", "w")

    build_windows(gb)
    try:
        render_control_status(gb, status)
    except Exception:
        pass
    t = start_status_display(gb, status)

    add_log_line(
        gb,
        "Starting servers on port %d and %d"
        % (int(config["port"]), int(config["port"] + 1)),
        CYAN,
    )

    add_log_line(gb, f"[manager] config {config}", CYAN)
    start_server(int(config["port"]), config["rpcpath"])

    # workaround for bug in curses: no resize messages
    os.environ["LINES"] = "blah"
    del os.environ["LINES"]
    os.environ["COLUMNS"] = "blah"
    del os.environ["COLUMNS"]

    try:
        while 1:
            c = gb.scr.getch()
            if c == curses.KEY_RESIZE:
                build_windows(gb)
                try:
                    render_control_status(gb, status)
                except Exception:
                    pass
            elif c == ord("q"):  # QUIT
                if engines.check_local_engine() or engines.count_cluster_engines() > 0:
                    ans = enter_command(
                        gb, "Are you SURE? Engines are STOPPED! (yes/no): "
                    )
                else:
                    ans = enter_command(gb, "Are you sure? (yes/no): ")
                if ans == "yes":
                    if (
                        engines.check_local_engine()
                        or engines.count_cluster_engines() > 0
                    ):
                        engines.stop_all()
                    stop_status_display(status, t)
                    gb.scr.refresh()
                    break
            elif c == ord("s"):  # START local engine
                cpus = get_number(gb, "Number of cpus (def=30): ", 30)
                mem = get_number(gb, "Maximal memory in GB (def=62): ", 62)
                if not cpus is None and not mem is None:
                    engines.start_local(gb, cpus, mem * 1024.0)

            elif c == ord("x"):  # STOP local engines
                engines.stop_local(gb)

            elif c == ord("d"):  # START SLURM engines
                on_snellius = "snellius" in zslurm_shared.get_full_hostname()
                if on_snellius:
                    default_partition = "genoa"
                else:
                    default_partition = "normal"

                partition = enter_command(
                    gb, f"Enter partition (default={default_partition}): "
                )
                if partition == "staging":
                    cmd = 1
                    cores = 1
                    constraint = ""
                else:
                    cmd = get_number(gb, "Number of engines (def=1): ", 1)
                    if on_snellius:
                        ans = enter_command(gb, "Reserve SSD scratch node? (Y/n): ")
                        if ans.lower() in ("", "y", "yes", "1"):
                            constraint = "scratch-node"
                        else:
                            constraint = ""
                    else:
                        constraint = enter_command(gb, "Enter feature (default=): ")
                    cores = get_number(
                        gb, "Enter number of cores (default=exclusive): ", 0
                    )

                runtime = enter_command(gb, "Enter time (default=5-0:0:0): ")

                if runtime == "":
                    runtime = "5-0:0:0"
                if partition == "":
                    partition = default_partition

                if not cmd is None and not cores is None:
                    engines.start_slurm(gb, cmd, runtime, partition, constraint, cores)
            elif c == ord("p"):
                jobpattern = enter_command(gb, "Enter job name pattern to prioritize: ")
                if not jobpattern == "":
                    jobs.prioritize(jobpattern)
            elif c == ord("n"):
                jobpattern = enter_command(
                    gb, "Enter job name pattern to deprioritize: "
                )
                if not jobpattern == "":
                    jobs.deprioritize(jobpattern)

            elif c == ord("1"):
                archive_total = get_number(
                    gb, "Enter new archive total capacity (in gb): ", 10000
                )
                if not archive_total is None:
                    jobs.archive_total = archive_total
            elif c == ord("2"):
                active_total = get_number(
                    gb, "Enter new active total capacity (in gb): ", 10000
                )
                if not active_total is None:
                    jobs.active_total = active_total
            elif c == ord("3"):
                dcache_total = get_number(
                    gb, "Enter new dcache total capacity (in gb): ", 10000
                )
                if not dcache_total is None:
                    jobs.dcache_total = dcache_total
            elif c == ord("4"):
                archive_inuse = get_number(
                    gb, "Enter new archive used capacity (in gb): ", 10000
                )
                if not archive_inuse is None:
                    jobs.archive_inuse = archive_inuse
            elif c == ord("5"):
                active_inuse = get_number(
                    gb, "Enter new active used capacity (in gb): ", 10000
                )
                if not active_inuse is None:
                    jobs.active_inuse = active_inuse
            elif c == ord("6"):
                dcache_inuse = get_number(
                    gb, "Enter new dcache used capacity (in gb): ", 10000
                )
                if not dcache_inuse is None:
                    jobs.dcache_inuse = dcache_inuse

            elif c == ord("c"):  # STOP PBS engines
                cmd = get_number(
                    gb, "Number of engines (def=all): ", engines.count_cluster_engines()
                )
                if not cmd is None:
                    engines.stop_slurm(gb, cmd)

            
            elif c == ord("a"):  # auto consolidate toggle
                status.autoconsolidate_enable = not status.autoconsolidate_enable
                add_log_line(
                    gb,
                    "Auto consolidate set to " + ("ON" if status.autoconsolidate_enable else "OFF"),
                    GREEN,
                )
                try:
                    render_control_status(gb, status)
                except Exception:
                    pass
            elif c == ord("l"):  # last in/first out toggle
                status.lastin_first = not status.lastin_first
                add_log_line(
                    gb,
                    "Last in / First out set to "
                    + ("ON" if status.lastin_first else "OFF"),
                    GREEN,
                )
                try:
                    render_control_status(gb, status)
                except Exception:
                    pass
            elif c == ord("m"):  # set context size to search for good fitting jobs
                cmd = get_number(gb, "Search context size for jobs [n=500]: ", 500)
                if not cmd is None:
                    status.prio_fillmem_context = max(int(cmd), 1)
                add_log_line(gb, "Fill memory context size set to " + str(cmd), GREEN)
                try:
                    render_control_status(gb, status)
                except Exception:
                    pass
            elif c == ord("g"):  # autogrow toggle
                status.autogrow_enable = not status.autogrow_enable
                if status.autogrow_enable:
                    current_cap = 40
                    try:
                        current_cap = int(status.config.get('autogrow_max_compute_nodes', 40))
                    except Exception:
                        current_cap = 40
                    cap = get_number(
                        gb,
                        f"Enter max compute autogrow nodes (default={current_cap}): ",
                        current_cap,
                    )
                    if cap is not None:
                        status.config['autogrow_max_compute_nodes'] = max(0, int(cap))
                add_log_line(
                    gb,
                    "Autogrow set to " + ("ON" if status.autogrow_enable else "OFF"),
                    GREEN,
                )
                if status.autogrow_enable:
                    add_log_line(
                        gb,
                        "Autogrow compute node cap set to " + str(status.config.get('autogrow_max_compute_nodes', 40)),
                        CYAN,
                    )
                try:
                    render_control_status(gb, status)
                except Exception:
                    pass
            elif c == ord("o"):
                cmd = enter_command(gb, "Enter name or [archive|compute] +/-nr:")
                r = engines.toggle_phasing_out(cmd)
                add_log_line(gb, r)

            elif c == ord("u"):  # Update ncurses screen
                gb.scr.refresh()
    except Exception:
        add_log_line(gb, "EXCEPTION!!! EMERGENCY SHUTDOWN IN PROGRESS!", RED)
        try:
            engines.stop_all()
            stop_status_display(status, t)
        except Exception:
            pass
        raise

    time.sleep(2)  # wait for all paint ops to finish and engines to terminate


curses.wrapper(main)
